{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI-QC - brainmask\n",
    "https://mriqc.readthedocs.io/en/latest/docker.html#docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it nipreps/mriqc:latest --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the participant level in subjects 001 002 003\n",
    "\n",
    "If the argument `--participant_label` is not provided, then all subjects will be processed and the group level analysis will automatically be executed without need of running the command in item 3.\n",
    "\n",
    "Paths `<bids_dir>` and `<output_dir>` must be absolute. In particular, specifying relative paths for `<output_dir>` will generate no error and mriqc will run to completion without error but produce no output.\n",
    "\n",
    "For security reasons, we recommend to run the docker command with the options `--read-only --tmpfs /run --tmpfs /tmp`. This will run the docker image in read-only mode, and map the temporary folders `/run` and `/tmp` to the temporal folder of the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# handle performance\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the group level and report generation on previously processed (use the same `<output_dir>`) subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v2_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop of individual participants\n",
    "And group report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(n_sub):\n",
    "    # if sub-xxx_T1w.html exists, skip\n",
    "    if os.path.isfile(f'{output_folder}/sub-{sub+1:03}_T1w.html'):\n",
    "        print(f'Subject {sub+1:03} already processed')\n",
    "        continue\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n",
    "\n",
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many .html file reports were generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_folder = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset'\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "\n",
    "# check how many .html files are in output folder\n",
    "n_html = len([f for f in os.listdir(output_folder) if f.endswith('.html')])\n",
    "print(f'Number of .html files in output folder: {n_html}')\n",
    "\n",
    "# which subjects have not been processed (range: 1-1210)\n",
    "sub_list = [f for f in range(1, n_sub+1) if not os.path.isfile(f'{output_folder}/sub-{f:03}_T1w.html')]\n",
    "print(f'Subjects not processed ({len(sub_list)}): {sub_list}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics correlation - old Meri's scores\n",
    "Meri's subjective scores vs IQMs (Image Quality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='samples_v3')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(meri, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% confidence interval: qi_2, summary_bg_mad, summary_gm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [qi_2, summary_bg_mad, summary_gm_mean]\n",
    "relevant_metrics_name = ['qi_2', 'summary_bg_mad', 'summary_gm_mean']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Brain QC (Thomas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# usage: qc_list_bids_csv [-h] [--mask-patterns MASK_PATTERNS [MASK_PATTERNS ...]] [--out-csv OUT_CSV] [--anonymize-name | --no-anonymize-name] bids-dir\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"_mask.nii.gz\"\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Thomas commands - branch mreye\n",
    "# being in the data folder\n",
    "qc_list_bids_csv . --mask-patterns-base derivatives/masks/ --out-csv bids.csv --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w # index the folder\n",
    "qc_generate_reports derivatives/reports bids.csv # generate the reports\n",
    "# mine\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output/ /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv # outpath bids.csv\n",
    "qc_generate_index /home/jaimebarranco/Desktop/fetal_qc_output/ # generate index.html\n",
    "# test\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids_test --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids_test/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output_test/ /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv # outpath bids.csv\n",
    "# pipeline\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MREye-QC - brain/eye masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# mreye-qc\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mreyeqc_output:/out mreyeqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12\n",
    "# 7bd99c588704826d399898ec1f7419d40dc09b27473d511066f3cf8ab097fe5a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/home/jaimebarranco/Desktop/samples_v3_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/mreyeqc_output'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(1, 6):\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation - old Meri's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mriqc_eye_mask')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% confidence interval: snr_csf, summary_csf_mean, summary_wm_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [snr_csf, summary_csf_mean, summary_wm_mad]\n",
    "relevant_metrics_name = ['snr_csf', 'summary_csf_mean', 'summary_wm_mad']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and Bene\n",
    "for i in range(len(metrics_meri)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(metrics_meri[i], metrics_bene[i])\n",
    "    # if p_value < ALPHA:\n",
    "    print(f'{metrics_names[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# Eyes closed correlation\n",
    "sum = 0\n",
    "for i in range(len(eyes_closed_meri)):\n",
    "    if eyes_closed_meri[i] == eyes_closed_bene[i]: sum += 1 \n",
    "print(f'eyes_closed: {np.round(sum/len(eyes_closed_meri)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical pie charts Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating_text'].to_list()\n",
    "rating_meri_exclude = rating_meri.count('exclude')\n",
    "rating_meri_poor = rating_meri.count('poor')\n",
    "rating_meri_acceptable = rating_meri.count('acceptable')\n",
    "rating_meri_excellent = rating_meri.count('excellent')\n",
    "rating_meri_count = [rating_meri_exclude, rating_meri_poor, rating_meri_acceptable, rating_meri_excellent]\n",
    "blur_meri = df_meri['blur_text'].to_list()\n",
    "blur_meri_low = blur_meri.count('low')\n",
    "blur_meri_moderate = blur_meri.count('moderate')\n",
    "blur_meri_high = blur_meri.count('high')\n",
    "blur_meri_count = [blur_meri_low, blur_meri_moderate, blur_meri_high]\n",
    "noise_meri = df_meri['noise_text'].to_list()\n",
    "noise_meri_low = noise_meri.count('low')\n",
    "noise_meri_moderate = noise_meri.count('moderate')\n",
    "noise_meri_high = noise_meri.count('high')\n",
    "noise_meri_count = [noise_meri_low, noise_meri_moderate, noise_meri_high]\n",
    "motion_meri = df_meri['motion_text'].to_list()\n",
    "motion_meri_low = motion_meri.count('low')\n",
    "motion_meri_moderate = motion_meri.count('moderate')\n",
    "motion_meri_high = motion_meri.count('high')\n",
    "motion_meri_count = [motion_meri_low, motion_meri_moderate, motion_meri_high]\n",
    "bgair_meri = df_meri['bgair_text'].to_list()\n",
    "bgair_meri_low = bgair_meri.count('low')\n",
    "bgair_meri_moderate = bgair_meri.count('moderate')\n",
    "bgair_meri_high = bgair_meri.count('high')\n",
    "bgair_meri_count = [bgair_meri_low, bgair_meri_moderate, bgair_meri_high]\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating_text'].to_list()\n",
    "rating_bene_exclude = rating_bene.count('exclude')\n",
    "rating_bene_poor = rating_bene.count('poor')\n",
    "rating_bene_acceptable = rating_bene.count('acceptable')\n",
    "rating_bene_excellent = rating_bene.count('excellent')\n",
    "rating_bene_count = [rating_bene_exclude, rating_bene_poor, rating_bene_acceptable, rating_bene_excellent]\n",
    "blur_bene = df_bene['blur_text'].to_list()\n",
    "blur_bene_low = blur_bene.count('low')\n",
    "blur_bene_moderate = blur_bene.count('moderate')\n",
    "blur_bene_high = blur_bene.count('high')\n",
    "blur_bene_count = [blur_bene_low, blur_bene_moderate, blur_bene_high]\n",
    "noise_bene = df_bene['noise_text'].to_list()\n",
    "noise_bene_low = noise_bene.count('low')\n",
    "noise_bene_moderate = noise_bene.count('moderate')\n",
    "noise_bene_high = noise_bene.count('high')\n",
    "noise_bene_count = [noise_bene_low, noise_bene_moderate, noise_bene_high]\n",
    "motion_bene = df_bene['motion_text'].to_list()\n",
    "motion_bene_low = motion_bene.count('low')\n",
    "motion_bene_moderate = motion_bene.count('moderate')\n",
    "motion_bene_high = motion_bene.count('high')\n",
    "motion_bene_count = [motion_bene_low, motion_bene_moderate, motion_bene_high]\n",
    "bgair_bene = df_bene['bgair_text'].to_list()\n",
    "bgair_bene_low = bgair_bene.count('low')\n",
    "bgair_bene_moderate = bgair_bene.count('moderate')\n",
    "bgair_bene_high = bgair_bene.count('high')\n",
    "bgair_bene_count = [bgair_bene_low, bgair_bene_moderate, bgair_bene_high]\n",
    "\n",
    "# Categories\n",
    "categories4 = ['exclude', 'poor', 'acceptable', 'excellent']\n",
    "categories3 = ['low', 'moderate', 'high']\n",
    "\n",
    "# Color palette\n",
    "# custom_colors = ['#F8D948', '#B847C4', '#4C9F38', '#3F75AA']\n",
    "custom_colors_4 = ['#dc3545', '#ffc107', '#0d6efd', '#198754']\n",
    "custom_colors_3 = ['#198754', '#ffc107', '#dc3545']\n",
    "\n",
    "# Pie chart - ratings\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(rating_meri_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(rating_bene_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Ratings')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# # Bar chart - ratings\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# ax[0].bar(categories4, rating_meri_count, color=custom_colors_4)\n",
    "# ax[0].set_title('Meri')\n",
    "# ax[1].bar(categories4, rating_bene_count, color=custom_colors_4)\n",
    "# ax[1].set_title('Bene')\n",
    "\n",
    "# # Adding title to the figure\n",
    "# fig.suptitle('Bar Chart - Ratings')\n",
    "# fig.tight_layout()\n",
    "# fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - blur\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(blur_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(blur_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Blur')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - noise\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(noise_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(noise_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Noise')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - motion\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(motion_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(motion_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Motion')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - bgair\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(bgair_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(bgair_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Background Air')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots Meri & Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/subjective_evaluations/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/subjective_evaluations/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "# scatter plots of the metrics (Meri vs Bene)\n",
    "for i in range(len(metrics_meri)):\n",
    "    # Create a joint plot with marginal distributions\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_palette(\"Set1\")  # Choose a color palette\n",
    "    # Create the joint plot\n",
    "    g = sns.jointplot(x=metrics_meri[i], y=metrics_bene[i], kind='scatter', color='b', s=40, edgecolor=\"skyblue\", linewidth=2)\n",
    "    # Annotate the axes with labels\n",
    "    g.ax_joint.set_xlabel(\"Meri\", fontsize=12)\n",
    "    g.ax_joint.set_ylabel(\"Bene\", fontsize=12)\n",
    "    plt.title(metrics_names[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation IQMs and subjective scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation with avg ratings (Meri and Bene)\n",
    "\n",
    "Metrics whose p-values are lower than 0.05 are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "MASK = 'brain' # 'brain' or 'eye'\n",
    "ARTIFACT = 'rating' # rating, blur, noise, motion, bgair\n",
    "CRITERIA = 0 # 0 or 1\n",
    "EXCLUDE = 1 # True or False, 1 or 0 IF CRITERIA = 1\n",
    "\n",
    "if MASK == 'brain':\n",
    "    sheet_name = 'brainmask_avg_data'\n",
    "elif MASK == 'eye':\n",
    "    sheet_name = 'mreyeqc_eyemask_avg'\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name=sheet_name)\n",
    "\n",
    "# rating subgroup\n",
    "df_rating = df.groupby([\"rating_text\"])\n",
    "\n",
    "# exclusion criteria\n",
    "if CRITERIA:\n",
    "    if EXCLUDE:\n",
    "        df_rating_exclude = df_rating.get_group(\"Exclude\")\n",
    "        df = df_rating_exclude # not to rewrite everything\n",
    "    else:\n",
    "        df_rating_exclude = df_rating.get_group(\"Exclude\")\n",
    "        df_rating_include = df[~df.index.isin(df_rating_exclude.index)] # Get the opposite of the group labeled \"Exclude\"\n",
    "        df = df_rating_include # not to rewrite everything\n",
    "\n",
    "# metrics\n",
    "rating = df[f'{ARTIFACT}'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [rating, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_names = [f'{ARTIFACT}', 'cjv', 'cnr', 'efc', 'fber', 'fwhm_avg', 'fwhm_x', 'fwhm_y', 'fwhm_z', 'icvs_csf', 'icvs_gm', 'icvs_wm', 'inu_med', 'inu_range', 'qi_1', 'qi_2', 'rpve_csf', 'rpve_gm', 'rpve_wm', 'snr_csf', 'snr_gm', 'snr_total', 'snr_wm', 'snrd_csf', 'snrd_gm', 'snrd_total', 'snrd_wm', 'summary_bg_k', 'summary_bg_mad', 'summary_bg_mean', 'summary_bg_median', 'summary_bg_n', 'summary_bg_p05', 'summary_bg_p95', 'summary_bg_stdv', 'summary_csf_k', 'summary_csf_mad', 'summary_csf_mean', 'summary_csf_median', 'summary_csf_n', 'summary_csf_p05', 'summary_csf_p95', 'summary_csf_stdv', 'summary_gm_k', 'summary_gm_mad', 'summary_gm_mean', 'summary_gm_median', 'summary_gm_n', 'summary_gm_p05', 'summary_gm_p95', 'summary_gm_stdv', 'summary_wm_k', 'summary_wm_mad', 'summary_wm_mean', 'summary_wm_median', 'summary_wm_n', 'summary_wm_p05', 'summary_wm_p95', 'summary_wm_stdv', 'tpm_overlap_csf', 'tpm_overlap_gm', 'tpm_overlap_wm', 'wm2max']\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "ALPHA = 0.05 # p_values below this value are considered significant\n",
    "metrics_dict = {} # Create a dictionary of arrays using a loop\n",
    "\n",
    "# Check if rating follows a normal distribution (Shapiro test)\n",
    "print(f'Rating: {stats.shapiro(rating)}')\n",
    "\n",
    "# plot rating distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.hist(rating, bins=4)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(metrics)):\n",
    "    # R measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    # null hypothesis: x comes from a normal distribution\n",
    "    # if p_value < alpha: The null hypothesis can be rejected --> The set may NOT follow a normal distribution\n",
    "    statistic, p_shapiro = stats.shapiro(metrics[i].flatten()) # check if it's a normal distribution\n",
    "    r, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA and p_shapiro > ALPHA:\n",
    "        metrics_dict[metrics_names[i]] = metrics[i]\n",
    "        print(f'{metrics_names[i]}: r = {r:.4}, p-value = {p_value:.4} || The set may follow a normal distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI95 - Subjects\n",
    "\n",
    "From the significant metrics extracted above, extract the outliers and see possible correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in metrics_dict.items() :\n",
    "    if key != f'{ARTIFACT}':\n",
    "        print(f'\\n{key}')\n",
    "        mean = np.mean(value)\n",
    "        std = np.std(value)\n",
    "        lower = mean - 2*std\n",
    "        upper = mean + 2*std\n",
    "        print(f'Lower: {lower}, Upper: {upper}')\n",
    "        for i in range(len(value)):\n",
    "            if value[i] < lower or value[i] > upper:\n",
    "                num_sub = df['sub'].values[i]\n",
    "                name_sub = df['name'].values[i].split('-')[1]\n",
    "                print(f'{num_sub:03} - {name_sub} - {value[i]} - {rating[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors of bad quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "train_test = True\n",
    "\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mreyeqc_brainmask_avg')\n",
    "\n",
    "target = 'rating'\n",
    "\n",
    "# metrics\n",
    "rating = df[f'{target}'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_names = ['cjv', 'cnr', 'efc', 'fber', 'fwhm_avg', 'fwhm_x', 'fwhm_y', 'fwhm_z', 'icvs_csf', 'icvs_gm', 'icvs_wm', 'inu_med', 'inu_range', 'qi_1', 'qi_2', 'rpve_csf', 'rpve_gm', 'rpve_wm', 'snr_csf', 'snr_gm', 'snr_total', 'snr_wm', 'snrd_csf', 'snrd_gm', 'snrd_total', 'snrd_wm', 'summary_bg_k', 'summary_bg_mad', 'summary_bg_mean', 'summary_bg_median', 'summary_bg_n', 'summary_bg_p05', 'summary_bg_p95', 'summary_bg_stdv', 'summary_csf_k', 'summary_csf_mad', 'summary_csf_mean', 'summary_csf_median', 'summary_csf_n', 'summary_csf_p05', 'summary_csf_p95', 'summary_csf_stdv', 'summary_gm_k', 'summary_gm_mad', 'summary_gm_mean', 'summary_gm_median', 'summary_gm_n', 'summary_gm_p05', 'summary_gm_p95', 'summary_gm_stdv', 'summary_wm_k', 'summary_wm_mad', 'summary_wm_mean', 'summary_wm_median', 'summary_wm_n', 'summary_wm_p05', 'summary_wm_p95', 'summary_wm_stdv', 'tpm_overlap_csf', 'tpm_overlap_gm', 'tpm_overlap_wm', 'wm2max']\n",
    "\n",
    "# Generate the training set.  Set random_state to be able to replicate results.\n",
    "if train_test:\n",
    "    train = df.sample(frac=0.8, random_state=1)\n",
    "else: \n",
    "    train = df    \n",
    "\n",
    "# Select anything not in the training set and put it in the testing set.\n",
    "test = df.loc[~df.index.isin(train.index)]\n",
    "\n",
    "# Print the shapes of both sets.\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "ALPHA = 0.05\n",
    "f_statistics, p_values = f_regression(train[metrics_names], train[target])\n",
    "\n",
    "# Print for which metrics_names the p-value is less than ALPHA\n",
    "for i in range(len(metrics_names)):\n",
    "    if p_values[i] < ALPHA:\n",
    "        print(metrics_names[i], f_statistics[i], p_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear regression\n",
    "\n",
    "# Import the linear models.\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sma\n",
    "\n",
    "# Initialize the model class.\n",
    "model = linear_model.LinearRegression()\n",
    "# model = linear_model.Ridge(alpha = 0.5)\n",
    "\n",
    "# Fit the model to the training data.\n",
    "# feature = train[[\"summary_wm_k\", \"icvs_csf\", \"summary_csf_p05\", \"tpm_overlap_csf\", \"summary_csf_median\", \"summary_wm_median\", \"snr_total\", \"summary_csf_mad\", \"summary_wm_k\", \"summary_wm_mean\", \"summary_wm_p95\", \"cjv\", \"cnr\", \"summary_gm_p95\"]]\n",
    "feature = train[globals()['features']]\n",
    "trained_model = model.fit(feature, train[target])\n",
    "\n",
    "# Model score, intercept and  slope\n",
    "intercept = trained_model.intercept_\n",
    "slope = trained_model.coef_\n",
    "print(f'R score = {trained_model.score(feature, train[target])} \\nIntercept = {intercept} \\nSlope = {slope}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Error\n",
    "\n",
    "# Import the scikit-learn function to compute error. Explained variance score.\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "if train_test:\n",
    "    # Generate our predictions for the test set.\n",
    "    # predictions = model.predict(test[[\"summary_wm_k\", \"icvs_csf\", \"summary_csf_p05\", \"tpm_overlap_csf\", \"summary_csf_median\", \"summary_wm_median\", \"snr_total\", \"summary_csf_mad\", \"summary_wm_k\", \"summary_wm_mean\", \"summary_wm_p95\", \"cjv\", \"cnr\", \"summary_gm_p95\"]])\n",
    "    predictions = model.predict(test[globals()['features']])\n",
    "\n",
    "    # Compute error between our test predictions and the actual values.\n",
    "    RMSE = mean_squared_error(predictions, test[target], squared=False)\n",
    "    print('Root mean squared error: ', RMSE)\n",
    "\n",
    "    # R score (it can be negative, if the model is arbitrarily worse)\n",
    "    print('Variance score (R-score): %.2f' % r2_score(test[target], predictions))\n",
    "\n",
    "else:\n",
    "    # Generate our predictions for the test set.\n",
    "    predictions = model.predict(train[[\"summary_wm_k\"]])\n",
    "\n",
    "    # Compute error between our test predictions and the actual values.\n",
    "    RMSE = mean_squared_error(predictions, train[target], squared=False)\n",
    "    print('Root mean squared error: ', RMSE)\n",
    "\n",
    "    # R score\n",
    "    print('Variance score (R-score): %.2f' % r2_score(train[target], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "if train_test:\n",
    "    plt.scatter(x=test[target], y=predictions,  color='red')\n",
    "else:\n",
    "    plt.scatter(x=train[target], y=predictions, color='red')\n",
    "plt.title('Linear Regression - Test Set')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Predicted Rating')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "y = df['rating']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=0)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestRegressor(n_estimators=100) #, random_state=0) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_r2 = scores.mean()\n",
    "std_r2 = scores.std()\n",
    "print(\"Mean R2 Score:\", mean_r2)\n",
    "print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# # Plot ROC curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC)')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor as Binary Classifier\n",
    "\n",
    "Convert the continuous values into Exclude/Include binary classes {0, 1} after predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "y = df['rating']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=0)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestRegressor(n_estimators=100) #, random_state=0) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_r2 = scores.mean()\n",
    "std_r2 = scores.std()\n",
    "print(\"Mean R2 Score:\", mean_r2)\n",
    "print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "# Define a threshold to classify instances\n",
    "threshold = 1  # Adjust this threshold as needed\n",
    "# Convert predicted values to binary classes using the threshold\n",
    "y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "# Evaluate the binary classification\n",
    "y_test_binary = (y_test < threshold).astype(int)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "# accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "confusion = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_binary) # not with the probabilities...\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier\n",
    "\n",
    "Change target to binary classification (0: Exclude, 1: Include). Then I can plot the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['exclusion']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=random_state) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "cross_val_scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='accuracy')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", cross_val_scores)\n",
    "# Calculate the mean accuracy from cross-validation\n",
    "mean_accuracy = cross_val_scores.mean()\n",
    "print(\"Mean Accuracy from Cross-Validation:\", mean_accuracy)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation (StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name', 'sub', 'subject', 'rating', 'rating_text', 'blur', 'blur_text', 'noise', 'noise_text', 'motion', 'motion_text', 'bgair', 'bgair_text', 'exclusion'])\n",
    "y = df['exclusion']\n",
    "\n",
    "# Create a Random Forest Classifier with random_state\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Initialize lists to store ROC curve values and AUC scores\n",
    "roc_curves = []\n",
    "auc_scores = []\n",
    "\n",
    "# Iterate over each fold and calculate ROC curve and AUC\n",
    "for train_idx, test_idx in StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predicted probabilities of the positive class\n",
    "    y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC for this fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# Plot ROC curves for each fold\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, (fpr, tpr) in enumerate(roc_curves):\n",
    "    plt.plot(fpr, tpr, lw=2, label='Fold %d (AUC = %0.2f)' % (i + 1, auc_scores[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Cross-Validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "target_names = ['excluded', 'included']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name', 'sub', 'subject', 'rating', 'rating_text', 'blur', 'blur_text', 'noise', 'noise_text', 'motion', 'motion_text', 'bgair', 'bgair_text', 'exclusion'])\n",
    "y = df['exclusion']\n",
    "\n",
    "# Create a Random Forest Classifier with random_state\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "for train_idx, test_idx in StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        rf_model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        name=f\"ROC fold {fold}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "        # plot_chance_level=(fold == n_splits - 1)\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "    fold+=1\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Positive label '{target_names[0]}')\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the feature importances (=1)\n",
    "print(feature_importances.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# avg\n",
    "mse_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "# Loop\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    # Create a Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=i) # random_state=0\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    mse_arr.append(mse)\n",
    "    # print(\"Mean Squared Error on Test Set:\", mse)\n",
    "    # Define a threshold to classify instances\n",
    "    threshold = 1  # Adjust this threshold as needed\n",
    "    # Convert predicted values to binary classes using the threshold\n",
    "    y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "    # Evaluate the binary classification\n",
    "    y_test_binary = (y_test < threshold).astype(int)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion)\n",
    "\n",
    "    # Dataframe for each iteration\n",
    "    globals()['feature_importance_df_%s' % i] = feature_importance_df\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "\n",
    "# Dataframe with the mean of the feature importances of all the iterations\n",
    "feature_importance_df_mean = pd.DataFrame({'Feature': X.columns, 'Importance': 0})\n",
    "for i in range(iter):\n",
    "    feature_importance_df_mean['Importance'] += globals()['feature_importance_df_%s' % i]['Importance']\n",
    "feature_importance_df_mean['Importance'] /= iter\n",
    "feature_importance_df_mean = feature_importance_df_mean.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df_mean)\n",
    "\n",
    "# results\n",
    "print('Mean of the mean squared error: ', np.mean(mse_arr))\n",
    "print('Mean of the accuracy: ', np.mean(acc_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the features with importance > 0.03 from the mean dataframe\n",
    "features_regressor = []\n",
    "for i in range(len(feature_importance_df_mean)):\n",
    "    if feature_importance_df_mean['Importance'].values[i] > 0.0:\n",
    "        features_regressor.append(feature_importance_df_mean['Feature'].values[i])\n",
    "print(features_regressor)\n",
    "\n",
    "first_features_regressor = features_regressor[:6]\n",
    "print(first_features_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['exclusion']\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# avg\n",
    "acc_arr = []\n",
    "\n",
    "# Loop\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    # Create a Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=i) # random_state=0\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_classifier.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    acc_arr.append(accuracy)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion)\n",
    "\n",
    "    # Dataframe for each iteration\n",
    "    globals()['feature_importance_df_%s' % i] = feature_importance_df\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "\n",
    "# Dataframe with the mean of the feature importances of all the iterations\n",
    "feature_importance_df_mean = pd.DataFrame({'Feature': X.columns, 'Importance': 0})\n",
    "for i in range(iter):\n",
    "    feature_importance_df_mean['Importance'] += globals()['feature_importance_df_%s' % i]['Importance']\n",
    "feature_importance_df_mean['Importance'] /= iter\n",
    "feature_importance_df_mean = feature_importance_df_mean.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df_mean)\n",
    "\n",
    "# results\n",
    "print('Mean accuracy: ', np.mean(acc_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the features with importance > 0.X from the mean dataframe\n",
    "features_classifier = []\n",
    "for i in range(len(feature_importance_df_mean)):\n",
    "    if feature_importance_df_mean['Importance'].values[i] > 0.03:\n",
    "        features_classifier.append(feature_importance_df_mean['Feature'].values[i])\n",
    "print(features_classifier)\n",
    "\n",
    "first_features_classifier = features_classifier[:7]\n",
    "print(first_features_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New random forest regressor with averaged importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "df['site'] = 'SHIP'\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_x = train_x.drop(columns=[\n",
    "    \"size_x\",\n",
    "    \"size_y\",\n",
    "    \"size_z\",\n",
    "    \"spacing_x\",\n",
    "    \"spacing_y\",\n",
    "    \"spacing_z\",\n",
    "    \"summary_bg_p05\", # all zeros\n",
    "])\n",
    "numeric_columns = train_x.columns.tolist()\n",
    "\n",
    "ratings = np.array([\"Exclude\"] * len(train_x))\n",
    "ratings[train_y.values < 1] = \"Exclude\"\n",
    "ratings[train_y.values >= 1] = \"Accept\"\n",
    "\n",
    "k_features = 6\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "sfs = SFS(\n",
    "    clf, \n",
    "    k_features=k_features, \n",
    "    forward=False, \n",
    "    verbose=0,\n",
    ")\n",
    "sfs = sfs.fit(train_x[numeric_columns], ratings)\n",
    "\n",
    "kept_features = list(sfs.k_feature_names_)\n",
    "print(f\"The {k_features} most important IQMs are {', '.join(sfs.k_feature_names_)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# random_state = 4\n",
    "# random_state = random.randint(0, 100)\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Loop\n",
    "mse_arr = []\n",
    "acc_arr = []\n",
    "prob_arr = []\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df[kept_features]\n",
    "    y = df['rating']\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    # Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=100, random_state=i) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_regressor, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - TRAIN THE MODEL - #\n",
    "    # Fit the model to the training data. Train the model\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_regressor.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "    # ---------------------- #\n",
    "\n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_regressor.predict(X_test)\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    mse_arr.append(mse)\n",
    "    # print(\"Mean Squared Error on Test Set:\", mse)\n",
    "    # Define a threshold to classify instances\n",
    "    threshold = 1  # Adjust this threshold as needed\n",
    "    # Convert predicted values to binary classes using the threshold\n",
    "    y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "    # Evaluate the binary classification\n",
    "    y_test_binary = (y_test < threshold).astype(int)\n",
    "    # Get predicted probabilities\n",
    "    # Convert predicted values to probabilities using the logistic function\n",
    "    # This approach might not be meaningful or appropriate, and it's usually better to use dedicated classification models or methods for computing class probabilities.\n",
    "    y_pred_prob = 1 - expit(y_pred_test)\n",
    "    prob_arr.append(y_pred_prob)\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion)\n",
    "    # --------------------- #\n",
    "\n",
    "avg_mse = np.mean(mse_arr)\n",
    "avg_acc = np.mean(acc_arr)\n",
    "avg_prob = np.mean(prob_arr, axis=0)\n",
    "print('Mean mse: ', avg_mse)\n",
    "print('Mean accuracy: ', avg_acc)\n",
    "print('Mean probs: ', avg_prob)\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, avg_prob) # not with the probabilities...\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New random forest classifier with avg importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "df['site'] = 'SHIP'\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_x = train_x.drop(columns=[\n",
    "    \"size_x\",\n",
    "    \"size_y\",\n",
    "    \"size_z\",\n",
    "    \"spacing_x\",\n",
    "    \"spacing_y\",\n",
    "    \"spacing_z\",\n",
    "    \"summary_bg_p05\", # all zeros\n",
    "])\n",
    "numeric_columns = train_x.columns.tolist()\n",
    "\n",
    "ratings = np.array([\"Exclude\"] * len(train_x))\n",
    "ratings[train_y.values < 1] = \"Exclude\"\n",
    "ratings[train_y.values >= 1] = \"Accept\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score to train_x and test_x for each column\n",
    "train_x_z = train_x.copy()\n",
    "test_x_z = test_x.copy()\n",
    "\n",
    "for col in numeric_columns:\n",
    "    mean = train_x[col].mean()\n",
    "    std = train_x[col].std()\n",
    "    train_x_z[col] = (train_x[col] - mean) / std\n",
    "    test_x_z[col] = (test_x[col] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_features = 6\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "sfs = SFS(\n",
    "    clf, \n",
    "    k_features=k_features, \n",
    "    forward=False, \n",
    "    verbose=0,\n",
    ")\n",
    "sfs = sfs.fit(train_x[numeric_columns], ratings)\n",
    "\n",
    "kept_features = list(sfs.k_feature_names_)\n",
    "print(f\"The {k_features} most important IQMs are {', '.join(sfs.k_feature_names_)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(sfs.k_feature_names_)\n",
    "\n",
    "#feature eliminated in order\n",
    "eliminated_feat = []\n",
    "for i in reversed(range(k_features, train_x[numeric_columns].shape[1]-1)):\n",
    "    feat0 = sfs.subsets_[i][\"feature_names\"]\n",
    "    feat1 = sfs.subsets_[i+1][\"feature_names\"]\n",
    "    el_feat = list(set(feat1) - set(feat0))\n",
    "    eliminated_feat.append(el_feat[0])\n",
    "\n",
    "print(\n",
    "    \"Here is the order in which the features have been eliminated \"\n",
    "    \"(1st in the list is the 1st feature to have been eliminated):\\n\"\n",
    "    f\"{', '.join(eliminated_feat)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept_features_aux = kept_features.copy()\n",
    "kept_features_aux.append(eliminated_feat[3])\n",
    "# kept_features_aux.pop(len(kept_features_aux)-1)\n",
    "print(f'{len(kept_features_aux)} features: {kept_features_aux}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "from scipy import stats\n",
    "\n",
    "# random_state = 0\n",
    "# random_state = random.randint(0, 100)\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Loop\n",
    "acc_arr = []\n",
    "prob_arr = []\n",
    "std_arr = []\n",
    "iter = 10\n",
    "for i in range(iter):\n",
    "    # Separate features (X) and target (y)\n",
    "    # X = df[features]\n",
    "    X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "    y = df['exclusion'] # 1 if excluded, 0 if included\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    X_columns = X.columns.tolist()\n",
    "\n",
    "    # Z-score to train_x and test_x for each column\n",
    "    train_x_z = X_train.copy()\n",
    "    test_x_z = X_test.copy()\n",
    "    for col in X_columns:\n",
    "        mean_train = X_train[col].mean()\n",
    "        std_train = X_train[col].std()\n",
    "        mean_test = X_test[col].mean()\n",
    "        std_test = X_test[col].std()\n",
    "        train_x_z[col] = (X_train[col] - mean_train) / std_train\n",
    "        test_x_z[col] = (X_test[col] - mean_test) / std_test\n",
    "    \n",
    "    X_train = train_x_z\n",
    "    X_test = test_x_z\n",
    "    \n",
    "    X_train = X_train.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        \"summary_bg_p05\", # all zeros\n",
    "        \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "    train_columns = X_train.columns.tolist()\n",
    "\n",
    "    X_test = X_test.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        \"summary_bg_p05\", # all zeros\n",
    "        \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "\n",
    "    # Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=i) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - TRAIN THE MODEL - #\n",
    "    # Fit the model to the training data. Train the model\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "    # ---------------------- #\n",
    "\n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_classifier.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Probabilities of the positive class = excluded (1)\n",
    "    prob_arr.append(y_pred_prob)\n",
    "    # print(\"Probabilities of being class 1 (excluded):\", y_pred_prob)\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion)\n",
    "    # standard deviation\n",
    "    std = np.std(acc_arr)\n",
    "    std_arr.append(std)\n",
    "    print(\"Standard Deviation:\", std)\n",
    "    # --------------------- #\n",
    "\n",
    "avg_acc = np.mean(acc_arr)\n",
    "avg_prob = np.mean(prob_arr, axis=0)\n",
    "avg_std = np.mean(std_arr)\n",
    "print('Mean accuracy: ', avg_acc)\n",
    "print('Mean probs: ', avg_prob)\n",
    "print('Mean std: ', avg_std)\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, avg_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t')\n",
    "df_aux = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "subjects = df_aux['subject'].tolist()\n",
    "for i in range(len(subjects)):\n",
    "    subjects[i] = subjects[i].split('.')[0]\n",
    "# subdataframe of df with the subjects not in subjects\n",
    "df = df[~df['subject_name'].isin(subjects)] # pure testing set\n",
    "\n",
    "# - INFERENCE - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_regressor.predict(df[kept_features])\n",
    "# Define a threshold to classify instances\n",
    "threshold = 1  # Adjust this threshold as needed\n",
    "# Convert predicted values to binary classes using the threshold\n",
    "y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "\n",
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_regressor = pd.DataFrame({'Subject': df['subject_name'], 'Predicted rating': y_pred_test})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_regressor[df_predictions_regressor['Predicted rating'] < 1]\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_regressor)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# Load dataset into a pandas DataFrame \n",
    "df = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t')\n",
    "df = df.sort_values(by='bids_name') # order by df['bids_name']\n",
    "df_names = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset/bids_csv.csv', sep=',')\n",
    "df_names['bids_name'] = df_names['im'].str.split('/').str[-1].str.split('.').str[0]\n",
    "df_names = df_names.sort_values(by='bids_name') # order by df_names['sub']\n",
    "df['name'] = df_names['name']\n",
    "# move last column to second position\n",
    "cols = list(df)\n",
    "cols.insert(1, cols.pop(cols.index('name')))\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "subs_ls = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset'\n",
    "files_1 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs_ls)) if not f.startswith('.')]\n",
    "files_1 = [int(f) for f in files_1] # files from string to int\n",
    "numbers_1 = [f for f in range(1, len(files_1)+1)]\n",
    "df_1 = pd.DataFrame({'Subject': files_1, 'Number': numbers_1})\n",
    "\n",
    "subs = '/home/jaimebarranco/Downloads/samples_v3'\n",
    "files_2 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs)) if not f.startswith('.')]\n",
    "files_2 = [int(f) for f in files_2] # files from string to int\n",
    "numbers_2 = [f for f in range(1, len(files_2)+1)]\n",
    "df_2 = pd.DataFrame({'Subject': files_2, 'Number': numbers_2})\n",
    "\n",
    "# df_1 only with df_2 subjects\n",
    "df_3 = df_1[df_1['Subject'].isin(df_2['Subject'])]\n",
    "numbers_3 = df_3['Number']\n",
    "bids_names = [f'sub-{n:03}_T1w' for n in numbers_3]\n",
    "df_3['bids_name'] = bids_names # add bids names\n",
    "\n",
    "df_aux = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "df_aux['bids_name'] = bids_names\n",
    "# move last column to second position\n",
    "cols = list(df_aux)\n",
    "cols.insert(1, cols.pop(cols.index('bids_name')))\n",
    "df_aux = df_aux.loc[:, cols]\n",
    "\n",
    "# subdataframe of df without the bids_names in df_aux\n",
    "df = df[~df['bids_name'].isin(df_aux['bids_name'])] # pure testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - INFERENCE - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_classifier.predict(df[train_columns])\n",
    "\n",
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_classifier = pd.DataFrame({'subject_name': df['name'], 'bids_name': df['bids_name'], 'pred_rating': y_pred_test})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_classifier[df_predictions_classifier['pred_rating'] == 1]\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_classifier)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save excluded dataframe to excel\n",
    "excluded.to_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded.xlsx', sheet_name='excluded', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the reports of the excluded subject into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "reports_folder = '/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset'\n",
    "output_folder = '/home/jaimebarranco/Downloads/excluded_subjects'\n",
    "\n",
    "# copy html reports from reports_folder that match the subjects in excluded dataframe to output_folder\n",
    "for i in range(len(excluded)):\n",
    "    subject = excluded['Subject'].values[i]\n",
    "    for filename in os.listdir(reports_folder):\n",
    "        if filename.startswith(f'{subject}_report'):\n",
    "            shutil.copy(f'{reports_folder}/{filename}', f'{output_folder}/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy remaining subjects to Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "import pandas as pd\n",
    "\n",
    "excluded_df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded_edited.xlsx', sheet_name='excluded') # excluded (new ones)\n",
    "reports_folder = '/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset'\n",
    "output_folder = '/home/jaimebarranco/Downloads/excluded_subjects_bene'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# get the list of subjects with 'my_rate' as 'nan'\n",
    "subjects = excluded_df[excluded_df['my_rate'].isna()]['subject_name'].tolist()\n",
    "\n",
    "# loop through the reports folder and copy the html reports for the matching subjects to the output folder\n",
    "for file_name in os.listdir(reports_folder):\n",
    "    if file_name.endswith(\".html\"):\n",
    "        subject = file_name.split(\"_\")[0]\n",
    "        if subject in subjects:\n",
    "            shutil.copy(os.path.join(reports_folder, file_name), os.path.join(output_folder, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More pure testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# Load dataset into a pandas DataFrame \n",
    "df = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t')\n",
    "df = df.sort_values(by='bids_name') # order by df['bids_name']\n",
    "df_names = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset/bids_csv.csv', sep=',')\n",
    "df_names['bids_name'] = df_names['im'].str.split('/').str[-1].str.split('.').str[0]\n",
    "df_names = df_names.sort_values(by='bids_name') # order by df_names['sub']\n",
    "df['name'] = df_names['name']\n",
    "# move last column to second position\n",
    "cols = list(df)\n",
    "cols.insert(1, cols.pop(cols.index('name')))\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "subs_ls = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset'\n",
    "files_1 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs_ls)) if not f.startswith('.')]\n",
    "files_1 = [int(f) for f in files_1] # files from string to int\n",
    "numbers_1 = [f for f in range(1, len(files_1)+1)]\n",
    "df_1 = pd.DataFrame({'Subject': files_1, 'Number': numbers_1})\n",
    "\n",
    "subs = '/home/jaimebarranco/Downloads/samples_v3'\n",
    "files_2 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs)) if not f.startswith('.')]\n",
    "files_2 = [int(f) for f in files_2] # files from string to int\n",
    "numbers_2 = [f for f in range(1, len(files_2)+1)]\n",
    "df_2 = pd.DataFrame({'Subject': files_2, 'Number': numbers_2})\n",
    "\n",
    "# df_1 only with df_2 subjects\n",
    "df_3 = df_1[df_1['Subject'].isin(df_2['Subject'])]\n",
    "numbers_3 = df_3['Number']\n",
    "bids_names = [f'sub-{n:03}_T1w' for n in numbers_3]\n",
    "df_3['bids_name'] = bids_names # add bids names\n",
    "\n",
    "df_aux = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "df_aux['bids_name'] = bids_names\n",
    "# move last column to second position\n",
    "cols = list(df_aux)\n",
    "cols.insert(1, cols.pop(cols.index('bids_name')))\n",
    "df_aux = df_aux.loc[:, cols]\n",
    "\n",
    "# subdataframe of df without the bids_names in df_aux\n",
    "df = df[~df['bids_name'].isin(df_aux['bids_name'])] # pure testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the table with the metrics of the excluded subjects and the old ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data') # old ones\n",
    "dfaux = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t') # IQMs\n",
    "df2 = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded_edited.xlsx', sheet_name='excluded') # excluded (new ones)\n",
    "# remove subjects without 'my_rate'\n",
    "df2_filtered = df2[(~df2['my_rate'].isna()) & (df2['my_rate'] != '')]\n",
    "# dfaux only with the subjects in df2 in column 'bids_name'\n",
    "dfaux_filtered = dfaux[dfaux['bids_name'].isin(df2_filtered['bids_name'])]\n",
    "# reset 'index' column of dfaux_filtered to add ratings\n",
    "dfaux_filtered = dfaux_filtered.reset_index(drop=True)\n",
    "# insert 'my_rate' column from df2_filtered to dfaux_filtered\n",
    "dfaux_filtered['rating'] = df2_filtered['my_rate']\n",
    "cols = list(dfaux_filtered)\n",
    "cols.insert(1, cols.pop(cols.index('rating')))\n",
    "dfaux_filtered = dfaux_filtered.loc[:, cols]\n",
    "\n",
    "df1_aux = df1.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "bids_names = df_3['bids_name']\n",
    "ratings = df1_aux['exclusion']\n",
    "df1_filtered = pd.DataFrame({'bids_name': bids_names, 'rating': ratings})\n",
    "# insert df1_aux into df1_filtered\n",
    "df1_aux = df1_aux.drop(columns=['exclusion'])\n",
    "df1_filtered = pd.concat([df1_filtered, df1_aux], axis=1)\n",
    "\n",
    "# join df1_filtered and dfaux_filtered\n",
    "df_merged = pd.concat([df1_filtered, dfaux_filtered], ignore_index=True)\n",
    "\n",
    "# large scale dataset = dfaux - df_merged\n",
    "df_ls_aux = dfaux.copy()\n",
    "df_ls_aux['name'] = df_names['name']\n",
    "df_ls = df_ls_aux[~df_ls_aux['bids_name'].isin(df_merged['bids_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'name' column from 'df'\n",
    "df_ls = df_ls.drop(columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ls['site'] = 'SHIP'\n",
    "# move 'site' column before 'cjv'\n",
    "cols = list(df_ls)\n",
    "cols.insert(1, cols.pop(cols.index('site')))\n",
    "df_ls = df_ls.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged to .tsv\n",
    "# df_ls.to_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/SHIP1027.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_merged.drop(columns=['rating','bids_name'])\n",
    "y = df_merged['rating']\n",
    "# df_merged['site'] = 'SHIP'\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_x = train_x.drop(columns=[\n",
    "    \"size_x\",\n",
    "    \"size_y\",\n",
    "    \"size_z\",\n",
    "    \"spacing_x\",\n",
    "    \"spacing_y\",\n",
    "    \"spacing_z\",\n",
    "    \"summary_bg_p05\", # all zeros\n",
    "])\n",
    "numeric_columns = train_x.columns.tolist()\n",
    "\n",
    "ratings = np.array([\"Exclude\"] * len(train_x))\n",
    "ratings[train_y.values < 1] = \"Exclude\"\n",
    "ratings[train_y.values >= 1] = \"Accept\"\n",
    "\n",
    "k_features = 5 # assuming 6 with the new dataset\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "sfs = SFS(\n",
    "    clf, \n",
    "    k_features=k_features, \n",
    "    forward=False, \n",
    "    verbose=0,\n",
    ")\n",
    "sfs = sfs.fit(train_x[numeric_columns], ratings)\n",
    "\n",
    "kept_features = list(sfs.k_feature_names_)\n",
    "print(f\"The {k_features} most important IQMs are {', '.join(sfs.k_feature_names_)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(sfs.k_feature_names_)\n",
    "\n",
    "#feature eliminated in order\n",
    "eliminated_feat = []\n",
    "for i in reversed(range(k_features, train_x[numeric_columns].shape[1]-1)):\n",
    "    feat0 = sfs.subsets_[i][\"feature_names\"]\n",
    "    feat1 = sfs.subsets_[i+1][\"feature_names\"]\n",
    "    el_feat = list(set(feat1) - set(feat0))\n",
    "    eliminated_feat.append(el_feat[0])\n",
    "\n",
    "print(\n",
    "    \"Here is the order in which the features have been eliminated \"\n",
    "    \"(1st in the list is the 1st feature to have been eliminated):\\n\"\n",
    "    f\"{', '.join(eliminated_feat)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept_features_aux = kept_features.copy()\n",
    "kept_features_aux.append(eliminated_feat[3])\n",
    "# kept_features_aux.pop(len(kept_features_aux)-1)\n",
    "print(f'{len(kept_features_aux)} features: {kept_features_aux}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# random_state = 0\n",
    "# random_state = random.randint(0, 100)\n",
    "\n",
    "# Loop\n",
    "acc_arr = []\n",
    "prob_arr = []\n",
    "iter = 10\n",
    "for i in range(iter):\n",
    "    # Separate features (X) and target (y)\n",
    "    # X = df_merged[numeric_columns]\n",
    "    X = df_merged.drop(columns=\n",
    "        [\n",
    "            'bids_name',\n",
    "            'rating',\n",
    "            \"cjv\",\n",
    "            \"cnr\",\n",
    "            # \"efc\",\n",
    "            # \"fber\",\n",
    "            # \"fwhm_avg\",\n",
    "            # \"fwhm_x\",\n",
    "            # \"fwhm_y\",\n",
    "            # \"fwhm_z\",\n",
    "            \"icvs_csf\",\n",
    "            \"icvs_gm\",\n",
    "            \"icvs_wm\",\n",
    "            # \"inu_med\",\n",
    "            # \"inu_range\",\n",
    "            # \"qi_1\",\n",
    "            # \"qi_2\",\n",
    "            \"rpve_csf\",\n",
    "            \"rpve_gm\",\n",
    "            \"rpve_wm\",\n",
    "            \"snr_csf\",\n",
    "            \"snr_gm\",\n",
    "            \"snr_total\",\n",
    "            \"snr_wm\",\n",
    "            \"snrd_csf\",\n",
    "            \"snrd_gm\",\n",
    "            # \"snrd_total\",\n",
    "            \"snrd_wm\",\n",
    "            \"summary_bg_k\",\n",
    "            \"summary_bg_mad\",\n",
    "            \"summary_bg_mean\",\n",
    "            \"summary_bg_median\",\n",
    "            \"summary_bg_n\",\n",
    "            \"summary_bg_p05\",\n",
    "            \"summary_bg_p95\",\n",
    "            \"summary_bg_stdv\",\n",
    "            \"summary_csf_k\",\n",
    "            \"summary_csf_mad\",\n",
    "            \"summary_csf_mean\",\n",
    "            \"summary_csf_median\",\n",
    "            \"summary_csf_n\",\n",
    "            \"summary_csf_p05\",\n",
    "            \"summary_csf_p95\",\n",
    "            \"summary_csf_stdv\",\n",
    "            \"summary_gm_k\",\n",
    "            \"summary_gm_mad\",\n",
    "            \"summary_gm_mean\",\n",
    "            \"summary_gm_median\",\n",
    "            \"summary_gm_n\",\n",
    "            \"summary_gm_p05\",\n",
    "            \"summary_gm_p95\",\n",
    "            \"summary_gm_stdv\",\n",
    "            \"summary_wm_k\",\n",
    "            \"summary_wm_mad\",\n",
    "            \"summary_wm_mean\",\n",
    "            \"summary_wm_median\",\n",
    "            \"summary_wm_n\",\n",
    "            \"summary_wm_p05\",\n",
    "            \"summary_wm_p95\",\n",
    "            \"summary_wm_stdv\",\n",
    "            \"tpm_overlap_csf\",\n",
    "            \"tpm_overlap_gm\",\n",
    "            \"tpm_overlap_wm\",\n",
    "            \"wm2max\"\n",
    "        ])\n",
    "    y = df_merged['rating'] # 1 if excluded, 0 if included\n",
    "\n",
    "    for col in X:\n",
    "        mean = X[col].mean()\n",
    "        std = X[col].std()\n",
    "        X[col] = (X[col] - mean) / std\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    X_columns = X.columns.tolist()\n",
    "\n",
    "    X_train = X_train.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        # \"summary_bg_p05\", # all zeros\n",
    "        # \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "    train_columns = X_train.columns.tolist()\n",
    "\n",
    "    X_test = X_test.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        # \"summary_bg_p05\", # all zeros\n",
    "        # \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "\n",
    "    # Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=i) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - TRAIN THE MODEL - #\n",
    "    # Fit the model to the training data. Train the model\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "    # ---------------------- #\n",
    "\n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_classifier.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Probabilities of the positive class = excluded (1)\n",
    "    prob_arr.append(y_pred_prob)\n",
    "    # print(\"Probabilities of being class 1 (excluded):\", y_pred_prob)\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion)\n",
    "    # --------------------- #\n",
    "\n",
    "avg_acc = np.mean(acc_arr)\n",
    "avg_prob = np.mean(prob_arr, axis=0)\n",
    "print('Mean accuracy: ', avg_acc)\n",
    "print('Mean probs: ', avg_prob)\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, avg_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large scale testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilities + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_scores = rf_classifier.predict_proba(df_ls[train_columns])[:, 1] # 0 for excluded according to the model training, or 1 if model trained with SHIP dataset\n",
    "print(f\"Median score: {np.median(y_scores):.3f}\")\n",
    "print(f\"P95 score: {np.percentile(y_scores, 95):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many values > threshold from y_scores\n",
    "threshold = 0.23\n",
    "count = (y_scores > threshold).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of values > threshold from y_scores\n",
    "y_pred_idx = (y_scores > threshold).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_classifier = pd.DataFrame({'bids_name': df_ls['bids_name'], 'subject_name': df_ls['name'], 'pred_rating': y_scores})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_classifier[df_predictions_classifier['pred_rating'] > threshold]\n",
    "excluded = excluded.drop(columns=['pred_rating'])\n",
    "excluded = excluded.sort_values(by=['subject_name'])\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_classifier)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - INFERENCE - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_classifier.predict(df_ls[train_columns])\n",
    "\n",
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_classifier = pd.DataFrame({'subject_name': df_ls['name'], 'bids_name': df_ls['bids_name'], 'pred_rating': y_pred_test})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_classifier[df_predictions_classifier['pred_rating'] == 1]\n",
    "excluded = excluded.sort_values(by=['name'])\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_classifier)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluded subjects to an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save excluded dataframe to excel\n",
    "excluded.to_excel('/home/jaimebarranco/Downloads/excluded_N183_NoBrainIQMs.xlsx', sheet_name='excluded', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the reports of the excluded subject into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "reports_folder = '/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset'\n",
    "output_folder = '/home/jaimebarranco/Downloads/excluded_N183_NoBrainIQMs'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# copy html reports from reports_folder that match the subjects in excluded dataframe to output_folder\n",
    "for i in range(len(excluded)):\n",
    "    subject = excluded['subject_name'].values[i]\n",
    "    for filename in os.listdir(reports_folder):\n",
    "        if filename.startswith(f'{subject}_report'):\n",
    "            shutil.copy(f'{reports_folder}/{filename}', f'{output_folder}/{filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-eye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
