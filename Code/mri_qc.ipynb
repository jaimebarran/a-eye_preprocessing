{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI-QC - brainmask\n",
    "https://mriqc.readthedocs.io/en/latest/docker.html#docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it nipreps/mriqc:latest --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the participant level in subjects 001 002 003\n",
    "\n",
    "If the argument `--participant_label` is not provided, then all subjects will be processed and the group level analysis will automatically be executed without need of running the command in item 3.\n",
    "\n",
    "Paths `<bids_dir>` and `<output_dir>` must be absolute. In particular, specifying relative paths for `<output_dir>` will generate no error and mriqc will run to completion without error but produce no output.\n",
    "\n",
    "For security reasons, we recommend to run the docker command with the options `--read-only --tmpfs /run --tmpfs /tmp`. This will run the docker image in read-only mode, and map the temporary folders `/run` and `/tmp` to the temporal folder of the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# handle performance\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the group level and report generation on previously processed (use the same `<output_dir>`) subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v2_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop of individual participants\n",
    "And group report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/home/jaimebarranco/Desktop/non_labeled_dataset_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/mriqc_non-labeled-dataset'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(271, n_sub):\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n",
    "\n",
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics correlation - old Meri's scores\n",
    "Meri's subjective scores vs IQMs (Image Quality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='samples_v3')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(meri, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% confidence interval: qi_2, summary_bg_mad, summary_gm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [qi_2, summary_bg_mad, summary_gm_mean]\n",
    "relevant_metrics_name = ['qi_2', 'summary_bg_mad', 'summary_gm_mean']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Brain QC (Thomas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# usage: qc_list_bids_csv [-h] [--mask-patterns MASK_PATTERNS [MASK_PATTERNS ...]] [--out-csv OUT_CSV] [--anonymize-name | --no-anonymize-name] bids-dir\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"_mask.nii.gz\"\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Thomas commands - branch mreye\n",
    "# being in the data folder\n",
    "qc_list_bids_csv . --mask-patterns-base derivatives/masks/ --out-csv bids.csv --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w # index the folder\n",
    "qc_generate_reports derivatives/reports bids.csv # generate the reports\n",
    "# mine\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output/ /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv # outpath bids.csv\n",
    "qc_generate_index /home/jaimebarranco/Desktop/fetal_qc_output/ # generate index.html\n",
    "# test\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids_test --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids_test/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output_test/ /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv # outpath bids.csv\n",
    "# pipeline\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MREye-QC - brain/eye masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# mreye-qc\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mreyeqc_output:/out mreyeqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12\n",
    "# 7bd99c588704826d399898ec1f7419d40dc09b27473d511066f3cf8ab097fe5a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/home/jaimebarranco/Desktop/samples_v3_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/mreyeqc_output'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(1, 6):\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics correlation - old Meri's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mriqc_eye_mask')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% confidence interval: snr_csf, summary_csf_mean, summary_wm_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [snr_csf, summary_csf_mean, summary_wm_mad]\n",
    "relevant_metrics_name = ['snr_csf', 'summary_csf_mean', 'summary_wm_mad']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and Bene\n",
    "for i in range(len(metrics_meri)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(metrics_meri[i], metrics_bene[i])\n",
    "    # if p_value < ALPHA:\n",
    "    print(f'{metrics_names[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# Eyes closed correlation\n",
    "sum = 0\n",
    "for i in range(len(eyes_closed_meri)):\n",
    "    if eyes_closed_meri[i] == eyes_closed_bene[i]: sum += 1 \n",
    "print(f'eyes_closed: {np.round(sum/len(eyes_closed_meri)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical pie charts Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating_text'].to_list()\n",
    "rating_meri_exclude = rating_meri.count('exclude')\n",
    "rating_meri_poor = rating_meri.count('poor')\n",
    "rating_meri_acceptable = rating_meri.count('acceptable')\n",
    "rating_meri_excellent = rating_meri.count('excellent')\n",
    "rating_meri_count = [rating_meri_exclude, rating_meri_poor, rating_meri_acceptable, rating_meri_excellent]\n",
    "blur_meri = df_meri['blur_text'].to_list()\n",
    "blur_meri_low = blur_meri.count('low')\n",
    "blur_meri_moderate = blur_meri.count('moderate')\n",
    "blur_meri_high = blur_meri.count('high')\n",
    "blur_meri_count = [blur_meri_low, blur_meri_moderate, blur_meri_high]\n",
    "noise_meri = df_meri['noise_text'].to_list()\n",
    "noise_meri_low = noise_meri.count('low')\n",
    "noise_meri_moderate = noise_meri.count('moderate')\n",
    "noise_meri_high = noise_meri.count('high')\n",
    "noise_meri_count = [noise_meri_low, noise_meri_moderate, noise_meri_high]\n",
    "motion_meri = df_meri['motion_text'].to_list()\n",
    "motion_meri_low = motion_meri.count('low')\n",
    "motion_meri_moderate = motion_meri.count('moderate')\n",
    "motion_meri_high = motion_meri.count('high')\n",
    "motion_meri_count = [motion_meri_low, motion_meri_moderate, motion_meri_high]\n",
    "bgair_meri = df_meri['bgair_text'].to_list()\n",
    "bgair_meri_low = bgair_meri.count('low')\n",
    "bgair_meri_moderate = bgair_meri.count('moderate')\n",
    "bgair_meri_high = bgair_meri.count('high')\n",
    "bgair_meri_count = [bgair_meri_low, bgair_meri_moderate, bgair_meri_high]\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating_text'].to_list()\n",
    "rating_bene_exclude = rating_bene.count('exclude')\n",
    "rating_bene_poor = rating_bene.count('poor')\n",
    "rating_bene_acceptable = rating_bene.count('acceptable')\n",
    "rating_bene_excellent = rating_bene.count('excellent')\n",
    "rating_bene_count = [rating_bene_exclude, rating_bene_poor, rating_bene_acceptable, rating_bene_excellent]\n",
    "blur_bene = df_bene['blur_text'].to_list()\n",
    "blur_bene_low = blur_bene.count('low')\n",
    "blur_bene_moderate = blur_bene.count('moderate')\n",
    "blur_bene_high = blur_bene.count('high')\n",
    "blur_bene_count = [blur_bene_low, blur_bene_moderate, blur_bene_high]\n",
    "noise_bene = df_bene['noise_text'].to_list()\n",
    "noise_bene_low = noise_bene.count('low')\n",
    "noise_bene_moderate = noise_bene.count('moderate')\n",
    "noise_bene_high = noise_bene.count('high')\n",
    "noise_bene_count = [noise_bene_low, noise_bene_moderate, noise_bene_high]\n",
    "motion_bene = df_bene['motion_text'].to_list()\n",
    "motion_bene_low = motion_bene.count('low')\n",
    "motion_bene_moderate = motion_bene.count('moderate')\n",
    "motion_bene_high = motion_bene.count('high')\n",
    "motion_bene_count = [motion_bene_low, motion_bene_moderate, motion_bene_high]\n",
    "bgair_bene = df_bene['bgair_text'].to_list()\n",
    "bgair_bene_low = bgair_bene.count('low')\n",
    "bgair_bene_moderate = bgair_bene.count('moderate')\n",
    "bgair_bene_high = bgair_bene.count('high')\n",
    "bgair_bene_count = [bgair_bene_low, bgair_bene_moderate, bgair_bene_high]\n",
    "\n",
    "# Categories\n",
    "categories4 = ['exclude', 'poor', 'acceptable', 'excellent']\n",
    "categories3 = ['low', 'moderate', 'high']\n",
    "\n",
    "# Color palette\n",
    "# custom_colors = ['#F8D948', '#B847C4', '#4C9F38', '#3F75AA']\n",
    "custom_colors_4 = ['#dc3545', '#ffc107', '#0d6efd', '#198754']\n",
    "custom_colors_3 = ['#198754', '#ffc107', '#dc3545']\n",
    "\n",
    "# Pie chart - ratings\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(rating_meri_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(rating_bene_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Ratings')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# # Bar chart - ratings\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# ax[0].bar(categories4, rating_meri_count, color=custom_colors_4)\n",
    "# ax[0].set_title('Meri')\n",
    "# ax[1].bar(categories4, rating_bene_count, color=custom_colors_4)\n",
    "# ax[1].set_title('Bene')\n",
    "\n",
    "# # Adding title to the figure\n",
    "# fig.suptitle('Bar Chart - Ratings')\n",
    "# fig.tight_layout()\n",
    "# fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - blur\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(blur_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(blur_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Blur')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - noise\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(noise_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(noise_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Noise')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - motion\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(motion_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(motion_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Motion')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - bgair\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(bgair_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(bgair_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Background Air')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots Meri & Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "# scatter plots of the metrics (Meri vs Bene)\n",
    "for i in range(len(metrics_meri)):\n",
    "    # Create a joint plot with marginal distributions\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_palette(\"Set1\")  # Choose a color palette\n",
    "    # Create the joint plot\n",
    "    g = sns.jointplot(x=metrics_meri[i], y=metrics_bene[i], kind='scatter', color='b', s=40, edgecolor=\"skyblue\", linewidth=2)\n",
    "    # Annotate the axes with labels\n",
    "    g.ax_joint.set_xlabel(\"Meri\", fontsize=12)\n",
    "    g.ax_joint.set_ylabel(\"Bene\", fontsize=12)\n",
    "    plt.title(metrics_names[i])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation IQMs and subjective scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation with avg ratings (Meri and Bene)\n",
    "\n",
    "Metrics whose p-values are lower than 0.05 are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "MASK = 'brain' # 'brain' or 'eye'\n",
    "ARTIFACT = 'rating' # rating, blur, noise, motion, bgair\n",
    "CRITERIA = 1 # 0 or 1\n",
    "EXCLUDE = 1 # True or False, 1 or 0 IF CRITERIA = 1\n",
    "\n",
    "if MASK == 'brain':\n",
    "    sheet_name = 'mreyeqc_brainmask_avg'\n",
    "elif MASK == 'eye':\n",
    "    sheet_name = 'mreyeqc_eyemask_avg'\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name=sheet_name)\n",
    "\n",
    "# rating subgroup\n",
    "df_rating = df.groupby([\"rating_text\"])\n",
    "\n",
    "# exclusion criteria\n",
    "if CRITERIA:\n",
    "    if EXCLUDE:\n",
    "        df_rating_exclude = df_rating.get_group(\"Exclude\")\n",
    "        df = df_rating_exclude # not to rewrite everything\n",
    "    else:\n",
    "        df_rating_exclude = df_rating.get_group(\"Exclude\")\n",
    "        df_rating_include = df[~df.index.isin(df_rating_exclude.index)] # Get the opposite of the group labeled \"Exclude\"\n",
    "        df = df_rating_include # not to rewrite everything\n",
    "\n",
    "# metrics\n",
    "rating = df[f'{ARTIFACT}'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [rating, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_names = [f'{ARTIFACT}', 'cjv', 'cnr', 'efc', 'fber', 'fwhm_avg', 'fwhm_x', 'fwhm_y', 'fwhm_z', 'icvs_csf', 'icvs_gm', 'icvs_wm', 'inu_med', 'inu_range', 'qi_1', 'qi_2', 'rpve_csf', 'rpve_gm', 'rpve_wm', 'snr_csf', 'snr_gm', 'snr_total', 'snr_wm', 'snrd_csf', 'snrd_gm', 'snrd_total', 'snrd_wm', 'summary_bg_k', 'summary_bg_mad', 'summary_bg_mean', 'summary_bg_median', 'summary_bg_n', 'summary_bg_p05', 'summary_bg_p95', 'summary_bg_stdv', 'summary_csf_k', 'summary_csf_mad', 'summary_csf_mean', 'summary_csf_median', 'summary_csf_n', 'summary_csf_p05', 'summary_csf_p95', 'summary_csf_stdv', 'summary_gm_k', 'summary_gm_mad', 'summary_gm_mean', 'summary_gm_median', 'summary_gm_n', 'summary_gm_p05', 'summary_gm_p95', 'summary_gm_stdv', 'summary_wm_k', 'summary_wm_mad', 'summary_wm_mean', 'summary_wm_median', 'summary_wm_n', 'summary_wm_p05', 'summary_wm_p95', 'summary_wm_stdv', 'tpm_overlap_csf', 'tpm_overlap_gm', 'tpm_overlap_wm', 'wm2max']\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "ALPHA = 0.05 # p_values below this value are considered significant\n",
    "metrics_dict = {} # Create a dictionary of arrays using a loop\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    # R measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    # null hypothesis: x comes from a normal distribution\n",
    "    # if p_value < alpha: The null hypothesis can be rejected --> The set may NOT follow a normal distribution\n",
    "    statistic, p_shapiro = stats.shapiro(metrics[i].flatten()) # check if it's a normal distribution\n",
    "    r, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA and p_shapiro > ALPHA:\n",
    "        metrics_dict[metrics_names[i]] = metrics[i]\n",
    "        print(f'{metrics_names[i]}: r = {r:.4}, p-value = {p_value:.4} || The set may follow a normal distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI95 - Subjects\n",
    "\n",
    "From the significant metrics extracted above, extract the outliers and see possible correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in metrics_dict.items() :\n",
    "    if key != f'{ARTIFACT}':\n",
    "        print(f'\\n{key}')\n",
    "        mean = np.mean(value)\n",
    "        std = np.std(value)\n",
    "        lower = mean - 2*std\n",
    "        upper = mean + 2*std\n",
    "        print(f'Lower: {lower}, Upper: {upper}')\n",
    "        for i in range(len(value)):\n",
    "            if value[i] < lower or value[i] > upper:\n",
    "                num_sub = df['sub'].values[i]\n",
    "                name_sub = df['name'].values[i].split('-')[1]\n",
    "                print(f'{num_sub:03} - {name_sub} - {value[i]} - {rating[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors of bad quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "train_test = True\n",
    "\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mreyeqc_brainmask_avg')\n",
    "\n",
    "target = 'rating'\n",
    "\n",
    "# metrics\n",
    "rating = df[f'{target}'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_names = ['cjv', 'cnr', 'efc', 'fber', 'fwhm_avg', 'fwhm_x', 'fwhm_y', 'fwhm_z', 'icvs_csf', 'icvs_gm', 'icvs_wm', 'inu_med', 'inu_range', 'qi_1', 'qi_2', 'rpve_csf', 'rpve_gm', 'rpve_wm', 'snr_csf', 'snr_gm', 'snr_total', 'snr_wm', 'snrd_csf', 'snrd_gm', 'snrd_total', 'snrd_wm', 'summary_bg_k', 'summary_bg_mad', 'summary_bg_mean', 'summary_bg_median', 'summary_bg_n', 'summary_bg_p05', 'summary_bg_p95', 'summary_bg_stdv', 'summary_csf_k', 'summary_csf_mad', 'summary_csf_mean', 'summary_csf_median', 'summary_csf_n', 'summary_csf_p05', 'summary_csf_p95', 'summary_csf_stdv', 'summary_gm_k', 'summary_gm_mad', 'summary_gm_mean', 'summary_gm_median', 'summary_gm_n', 'summary_gm_p05', 'summary_gm_p95', 'summary_gm_stdv', 'summary_wm_k', 'summary_wm_mad', 'summary_wm_mean', 'summary_wm_median', 'summary_wm_n', 'summary_wm_p05', 'summary_wm_p95', 'summary_wm_stdv', 'tpm_overlap_csf', 'tpm_overlap_gm', 'tpm_overlap_wm', 'wm2max']\n",
    "\n",
    "# Generate the training set.  Set random_state to be able to replicate results.\n",
    "if train_test:\n",
    "    train = df.sample(frac=0.8, random_state=1)\n",
    "else: \n",
    "    train = df    \n",
    "\n",
    "# Select anything not in the training set and put it in the testing set.\n",
    "test = df.loc[~df.index.isin(train.index)]\n",
    "\n",
    "# Print the shapes of both sets.\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "ALPHA = 0.05\n",
    "f_statistics, p_values = f_regression(train[metrics_names], train[target])\n",
    "\n",
    "# Print for which metrics_names the p-value is less than ALPHA\n",
    "for i in range(len(metrics_names)):\n",
    "    if p_values[i] < ALPHA:\n",
    "        print(metrics_names[i], f_statistics[i], p_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear regression\n",
    "\n",
    "# Import the linear models.\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sma\n",
    "\n",
    "# Initialize the model class.\n",
    "model = linear_model.LinearRegression()\n",
    "# model = linear_model.Ridge(alpha = 0.5)\n",
    "\n",
    "# Fit the model to the training data.\n",
    "# feature = train[[\"summary_wm_k\", \"icvs_csf\", \"summary_csf_p05\", \"tpm_overlap_csf\", \"summary_csf_median\", \"summary_wm_median\", \"snr_total\", \"summary_csf_mad\", \"summary_wm_k\", \"summary_wm_mean\", \"summary_wm_p95\", \"cjv\", \"cnr\", \"summary_gm_p95\"]]\n",
    "feature = train[globals()['features']]\n",
    "trained_model = model.fit(feature, train[target])\n",
    "\n",
    "# Model score, intercept and  slope\n",
    "intercept = trained_model.intercept_\n",
    "slope = trained_model.coef_\n",
    "print(f'R score = {trained_model.score(feature, train[target])} \\nIntercept = {intercept} \\nSlope = {slope}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Error\n",
    "\n",
    "# Import the scikit-learn function to compute error. Explained variance score.\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "if train_test:\n",
    "    # Generate our predictions for the test set.\n",
    "    # predictions = model.predict(test[[\"summary_wm_k\", \"icvs_csf\", \"summary_csf_p05\", \"tpm_overlap_csf\", \"summary_csf_median\", \"summary_wm_median\", \"snr_total\", \"summary_csf_mad\", \"summary_wm_k\", \"summary_wm_mean\", \"summary_wm_p95\", \"cjv\", \"cnr\", \"summary_gm_p95\"]])\n",
    "    predictions = model.predict(test[globals()['features']])\n",
    "\n",
    "    # Compute error between our test predictions and the actual values.\n",
    "    RMSE = mean_squared_error(predictions, test[target], squared=False)\n",
    "    print('Root mean squared error: ', RMSE)\n",
    "\n",
    "    # R score (it can be negative, if the model is arbitrarily worse)\n",
    "    print('Variance score (R-score): %.2f' % r2_score(test[target], predictions))\n",
    "\n",
    "else:\n",
    "    # Generate our predictions for the test set.\n",
    "    predictions = model.predict(train[[\"summary_wm_k\"]])\n",
    "\n",
    "    # Compute error between our test predictions and the actual values.\n",
    "    RMSE = mean_squared_error(predictions, train[target], squared=False)\n",
    "    print('Root mean squared error: ', RMSE)\n",
    "\n",
    "    # R score\n",
    "    print('Variance score (R-score): %.2f' % r2_score(train[target], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "if train_test:\n",
    "    plt.scatter(x=test[target], y=predictions,  color='red')\n",
    "else:\n",
    "    plt.scatter(x=train[target], y=predictions, color='red')\n",
    "plt.title('Linear Regression - Test Set')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Predicted Rating')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "y = df['rating']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=0)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestRegressor(n_estimators=100) #, random_state=0) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_r2 = scores.mean()\n",
    "std_r2 = scores.std()\n",
    "print(\"Mean R2 Score:\", mean_r2)\n",
    "print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# # Plot ROC curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC)')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier\n",
    "\n",
    "Change target to binary classification (0: Exclude, 1: Include). Then I can plot the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['exclusion']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=random_state) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "cross_val_scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='accuracy')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean accuracy from cross-validation\n",
    "mean_accuracy = cross_val_scores.mean()\n",
    "print(\"Mean Accuracy from Cross-Validation:\", mean_accuracy)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation (StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name', 'sub', 'subject', 'rating', 'rating_text', 'blur', 'blur_text', 'noise', 'noise_text', 'motion', 'motion_text', 'bgair', 'bgair_text', 'exclusion'])\n",
    "y = df['exclusion']\n",
    "\n",
    "# Create a Random Forest Classifier with random_state\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Initialize lists to store ROC curve values and AUC scores\n",
    "roc_curves = []\n",
    "auc_scores = []\n",
    "\n",
    "# Iterate over each fold and calculate ROC curve and AUC\n",
    "for train_idx, test_idx in StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predicted probabilities of the positive class\n",
    "    y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC for this fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# Plot ROC curves for each fold\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, (fpr, tpr) in enumerate(roc_curves):\n",
    "    plt.plot(fpr, tpr, lw=2, label='Fold %d (AUC = %0.2f)' % (i + 1, auc_scores[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Cross-Validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "target_names = ['excluded', 'included']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name', 'sub', 'subject', 'rating', 'rating_text', 'blur', 'blur_text', 'noise', 'noise_text', 'motion', 'motion_text', 'bgair', 'bgair_text', 'exclusion'])\n",
    "y = df['exclusion']\n",
    "\n",
    "# Create a Random Forest Classifier with random_state\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "for train_idx, test_idx in StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        rf_model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        name=f\"ROC fold {fold}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "        # plot_chance_level=(fold == n_splits - 1)\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "    fold+=1\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Positive label '{target_names[0]}')\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the feature importances (=1)\n",
    "print(feature_importances.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "\n",
    "# Loop\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    # Create a Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=i) # random_state=0\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print(\"Mean Squared Error on Test Set:\", mse)\n",
    "\n",
    "    # Dataframe for each iteration\n",
    "    globals()['feature_importance_df_%s' % i] = feature_importance_df\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "\n",
    "# Dataframe with the mean of the feature importances of all the iterations\n",
    "feature_importance_df_mean = pd.DataFrame({'Feature': X.columns, 'Importance': 0})\n",
    "for i in range(iter):\n",
    "    feature_importance_df_mean['Importance'] += globals()['feature_importance_df_%s' % i]['Importance']\n",
    "feature_importance_df_mean['Importance'] /= iter\n",
    "feature_importance_df_mean = feature_importance_df_mean.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df_mean)\n",
    "\n",
    "# list of the features with importance > 0.03 from the mean dataframe\n",
    "features = []\n",
    "for i in range(len(feature_importance_df_mean)):\n",
    "    if feature_importance_df_mean['Importance'].values[i] > 0.03:\n",
    "        features.append(feature_importance_df_mean['Feature'].values[i])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New random forest regressor with averaged importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df[features]\n",
    "y = df['rating']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=0)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestRegressor(n_estimators=100) #, random_state=0) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_r2 = scores.mean()\n",
    "std_r2 = scores.std()\n",
    "print(\"Mean R2 Score:\", mean_r2)\n",
    "print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# # Plot ROC curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC)')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-eye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
