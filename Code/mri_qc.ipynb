{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI-QC - brainmask\n",
    "https://mriqc.readthedocs.io/en/latest/docker.html#docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it nipreps/mriqc:latest --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the participant level in subjects 001 002 003\n",
    "\n",
    "If the argument `--participant_label` is not provided, then all subjects will be processed and the group level analysis will automatically be executed without need of running the command in item 3.\n",
    "\n",
    "Paths `<bids_dir>` and `<output_dir>` must be absolute. In particular, specifying relative paths for `<output_dir>` will generate no error and mriqc will run to completion without error but produce no output.\n",
    "\n",
    "For security reasons, we recommend to run the docker command with the options `--read-only --tmpfs /run --tmpfs /tmp`. This will run the docker image in read-only mode, and map the temporary folders `/run` and `/tmp` to the temporal folder of the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# handle performance\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the group level and report generation on previously processed (use the same `<output_dir>`) subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v2_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop of individual participants\n",
    "And group report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/home/jaimebarranco/Desktop/samples_v3_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/mriqc_output'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(6, n_sub):\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n",
    "\n",
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics correlation - old Meri's scores\n",
    "Meri's subjective scores vs IQMs (Image Quality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='samples_v3')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(meri, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% confidence interval: qi_2, summary_bg_mad, summary_gm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [qi_2, summary_bg_mad, summary_gm_mean]\n",
    "relevant_metrics_name = ['qi_2', 'summary_bg_mad', 'summary_gm_mean']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Brain QC (Thomas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# usage: qc_list_bids_csv [-h] [--mask-patterns MASK_PATTERNS [MASK_PATTERNS ...]] [--out-csv OUT_CSV] [--anonymize-name | --no-anonymize-name] bids-dir\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"_mask.nii.gz\"\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Thomas commands - branch mreye\n",
    "# being in the data folder\n",
    "qc_list_bids_csv . --mask-patterns-base derivatives/masks/ --out-csv bids.csv --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w # index the folder\n",
    "qc_generate_reports derivatives/reports bids.csv # generate the reports\n",
    "# mine\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output/ /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv # outpath bids.csv\n",
    "qc_generate_index /home/jaimebarranco/Desktop/fetal_qc_output/ # generate index.html\n",
    "# test\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids_test --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids_test/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output_test/ /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv # outpath bids.csv\n",
    "# pipeline\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MREye-QC - eyemask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# mreye-qc\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mreyeqc_output:/out mreyeqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12\n",
    "# 7bd99c588704826d399898ec1f7419d40dc09b27473d511066f3cf8ab097fe5a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/home/jaimebarranco/Desktop/samples_v3_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/mreyeqc_output'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(1, 6):\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics correlation - old Meri's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mriqc_eye_mask')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% confidence interval: snr_csf, summary_csf_mean, summary_wm_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [snr_csf, summary_csf_mean, summary_wm_mad]\n",
    "relevant_metrics_name = ['snr_csf', 'summary_csf_mean', 'summary_wm_mad']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and Bene\n",
    "for i in range(len(metrics_meri)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(metrics_meri[i], metrics_bene[i])\n",
    "    # if p_value < ALPHA:\n",
    "    print(f'{metrics_names[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# Eyes closed correlation\n",
    "sum = 0\n",
    "for i in range(len(eyes_closed_meri)):\n",
    "    if eyes_closed_meri[i] == eyes_closed_bene[i]: sum += 1 \n",
    "print(f'eyes_closed: {np.round(sum/len(eyes_closed_meri)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical pie charts Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating_text'].to_list()\n",
    "rating_meri_exclude = rating_meri.count('exclude')\n",
    "rating_meri_poor = rating_meri.count('poor')\n",
    "rating_meri_acceptable = rating_meri.count('acceptable')\n",
    "rating_meri_excellent = rating_meri.count('excellent')\n",
    "rating_meri_count = [rating_meri_exclude, rating_meri_poor, rating_meri_acceptable, rating_meri_excellent]\n",
    "blur_meri = df_meri['blur_text'].to_list()\n",
    "blur_meri_low = blur_meri.count('low')\n",
    "blur_meri_moderate = blur_meri.count('moderate')\n",
    "blur_meri_high = blur_meri.count('high')\n",
    "blur_meri_count = [blur_meri_low, blur_meri_moderate, blur_meri_high]\n",
    "noise_meri = df_meri['noise_text'].to_list()\n",
    "noise_meri_low = noise_meri.count('low')\n",
    "noise_meri_moderate = noise_meri.count('moderate')\n",
    "noise_meri_high = noise_meri.count('high')\n",
    "noise_meri_count = [noise_meri_low, noise_meri_moderate, noise_meri_high]\n",
    "motion_meri = df_meri['motion_text'].to_list()\n",
    "motion_meri_low = motion_meri.count('low')\n",
    "motion_meri_moderate = motion_meri.count('moderate')\n",
    "motion_meri_high = motion_meri.count('high')\n",
    "motion_meri_count = [motion_meri_low, motion_meri_moderate, motion_meri_high]\n",
    "bgair_meri = df_meri['bgair_text'].to_list()\n",
    "bgair_meri_low = bgair_meri.count('low')\n",
    "bgair_meri_moderate = bgair_meri.count('moderate')\n",
    "bgair_meri_high = bgair_meri.count('high')\n",
    "bgair_meri_count = [bgair_meri_low, bgair_meri_moderate, bgair_meri_high]\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating_text'].to_list()\n",
    "rating_bene_exclude = rating_bene.count('exclude')\n",
    "rating_bene_poor = rating_bene.count('poor')\n",
    "rating_bene_acceptable = rating_bene.count('acceptable')\n",
    "rating_bene_excellent = rating_bene.count('excellent')\n",
    "rating_bene_count = [rating_bene_exclude, rating_bene_poor, rating_bene_acceptable, rating_bene_excellent]\n",
    "blur_bene = df_bene['blur_text'].to_list()\n",
    "blur_bene_low = blur_bene.count('low')\n",
    "blur_bene_moderate = blur_bene.count('moderate')\n",
    "blur_bene_high = blur_bene.count('high')\n",
    "blur_bene_count = [blur_bene_low, blur_bene_moderate, blur_bene_high]\n",
    "noise_bene = df_bene['noise_text'].to_list()\n",
    "noise_bene_low = noise_bene.count('low')\n",
    "noise_bene_moderate = noise_bene.count('moderate')\n",
    "noise_bene_high = noise_bene.count('high')\n",
    "noise_bene_count = [noise_bene_low, noise_bene_moderate, noise_bene_high]\n",
    "motion_bene = df_bene['motion_text'].to_list()\n",
    "motion_bene_low = motion_bene.count('low')\n",
    "motion_bene_moderate = motion_bene.count('moderate')\n",
    "motion_bene_high = motion_bene.count('high')\n",
    "motion_bene_count = [motion_bene_low, motion_bene_moderate, motion_bene_high]\n",
    "bgair_bene = df_bene['bgair_text'].to_list()\n",
    "bgair_bene_low = bgair_bene.count('low')\n",
    "bgair_bene_moderate = bgair_bene.count('moderate')\n",
    "bgair_bene_high = bgair_bene.count('high')\n",
    "bgair_bene_count = [bgair_bene_low, bgair_bene_moderate, bgair_bene_high]\n",
    "\n",
    "# Categories\n",
    "categories4 = ['exclude', 'poor', 'acceptable', 'excellent']\n",
    "categories3 = ['low', 'moderate', 'high']\n",
    "\n",
    "# Color palette\n",
    "# custom_colors = ['#F8D948', '#B847C4', '#4C9F38', '#3F75AA']\n",
    "custom_colors_4 = ['#dc3545', '#ffc107', '#0d6efd', '#198754']\n",
    "custom_colors_3 = ['#198754', '#ffc107', '#dc3545']\n",
    "\n",
    "# Pie chart - ratings\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(rating_meri_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(rating_bene_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Ratings')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# # Bar chart - ratings\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# ax[0].bar(categories4, rating_meri_count, color=custom_colors_4)\n",
    "# ax[0].set_title('Meri')\n",
    "# ax[1].bar(categories4, rating_bene_count, color=custom_colors_4)\n",
    "# ax[1].set_title('Bene')\n",
    "\n",
    "# # Adding title to the figure\n",
    "# fig.suptitle('Bar Chart - Ratings')\n",
    "# fig.tight_layout()\n",
    "# fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - blur\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(blur_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(blur_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Blur')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - noise\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(noise_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(noise_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Noise')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - motion\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(motion_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(motion_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Motion')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - bgair\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(bgair_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(bgair_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Background Air')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plots Meri & Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "# scatter plots of the metrics (Meri vs Bene)\n",
    "for i in range(len(metrics_meri)):\n",
    "    # Create a joint plot with marginal distributions\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_palette(\"Set1\")  # Choose a color palette\n",
    "    # Create the joint plot\n",
    "    g = sns.jointplot(x=metrics_meri[i], y=metrics_bene[i], kind='scatter', color='b', s=40, edgecolor=\"skyblue\", linewidth=2)\n",
    "    # Annotate the axes with labels\n",
    "    g.ax_joint.set_xlabel(\"Meri\", fontsize=12)\n",
    "    g.ax_joint.set_ylabel(\"Bene\", fontsize=12)\n",
    "    plt.title(metrics_names[i])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainmask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation with avg ratings (Meri and Bene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mreyeqc_brainmask_avg')\n",
    "\n",
    "# metrics\n",
    "rating = df['rating'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [rating, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(rating, metrics[i])\n",
    "#     plt.xlabel('rating')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [summary_bg_median, summary_gm_mean, summary_gm_n, summary_wm_k, summary_wm_mean]\n",
    "relevant_metrics_name = ['summary_bg_median', 'summary_gm_mean', 'summary_gm_n', 'summary_wm_k', 'summary_wm_mean']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]} - {rating[j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eyemask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation avg ratings (Meri and Bene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mreyeqc_eyemask_avg')\n",
    "\n",
    "# metrics\n",
    "rating = df['rating'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [rating, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(rating, metrics[i])\n",
    "#     plt.xlabel('rating')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [summary_csf_mean]\n",
    "relevant_metrics_name = ['summary_csf_mean']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]} - {rating[j]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-eye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
