{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI-QC - brainmask\n",
    "https://mriqc.readthedocs.io/en/latest/docker.html#docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it nipreps/mriqc:latest --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the participant level in subjects 001 002 003\n",
    "\n",
    "If the argument `--participant_label` is not provided, then all subjects will be processed and the group level analysis will automatically be executed without need of running the command in item 3.\n",
    "\n",
    "Paths `<bids_dir>` and `<output_dir>` must be absolute. In particular, specifying relative paths for `<output_dir>` will generate no error and mriqc will run to completion without error but produce no output.\n",
    "\n",
    "For security reasons, we recommend to run the docker command with the options `--read-only --tmpfs /run --tmpfs /tmp`. This will run the docker image in read-only mode, and map the temporary folders `/run` and `/tmp` to the temporal folder of the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 002 003\n",
    "# handle performance\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the group level and report generation on previously processed (use the same `<output_dir>`) subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "docker run -it --rm -v <bids_dir>:/data:ro -v <output_dir>:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp\n",
    "# with my paths\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v2_bids:/data:ro -v /home/jaimebarranco/Desktop/mriqc_output:/out nipreps/mriqc:latest /data /out group #--read-only --tmpfs /run --tmpfs /tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop of individual participants\n",
    "And group report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(n_sub):\n",
    "    # if sub-xxx_T1w.html exists, skip\n",
    "    if os.path.isfile(f'{output_folder}/sub-{sub+1:03}_T1w.html'):\n",
    "        print(f'Subject {sub+1:03} already processed')\n",
    "        continue\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n",
    "\n",
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out nipreps/mriqc:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many .html file reports were generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_folder = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset'\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "\n",
    "# check how many .html files are in output folder\n",
    "n_html = len([f for f in os.listdir(output_folder) if f.endswith('.html')])\n",
    "print(f'Number of .html files in output folder: {n_html}')\n",
    "\n",
    "# which subjects have not been processed (range: 1-1210)\n",
    "sub_list = [f for f in range(1, n_sub+1) if not os.path.isfile(f'{output_folder}/sub-{f:03}_T1w.html')]\n",
    "print(f'Subjects not processed ({len(sub_list)}): {sub_list}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics correlation - old Meri's scores\n",
    "Meri's subjective scores vs IQMs (Image Quality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='samples_v3')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(meri, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% confidence interval: qi_2, summary_bg_mad, summary_gm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [qi_2, summary_bg_mad, summary_gm_mean]\n",
    "relevant_metrics_name = ['qi_2', 'summary_bg_mad', 'summary_gm_mean']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetal Brain QC (Thomas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# usage: qc_list_bids_csv [-h] [--mask-patterns MASK_PATTERNS [MASK_PATTERNS ...]] [--out-csv OUT_CSV] [--anonymize-name | --no-anonymize-name] bids-dir\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids\n",
    "qc_list_bids_csv --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv /home/jaimebarranco/Desktop/samples_v3_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"_mask.nii.gz\"\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Thomas commands - branch mreye\n",
    "# being in the data folder\n",
    "qc_list_bids_csv . --mask-patterns-base derivatives/masks/ --out-csv bids.csv --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w # index the folder\n",
    "qc_generate_reports derivatives/reports bids.csv # generate the reports\n",
    "# mine\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output/ /home/jaimebarranco/Desktop/fetal_qc_output/bids_csv.csv # outpath bids.csv\n",
    "qc_generate_index /home/jaimebarranco/Desktop/fetal_qc_output/ # generate index.html\n",
    "# test\n",
    "qc_list_bids_csv /home/jaimebarranco/Desktop/samples_v3_bids_test --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids_test/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w --out-csv /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv\n",
    "qc_generate_reports /home/jaimebarranco/Desktop/fetal_qc_output_test/ /home/jaimebarranco/Desktop/fetal_qc_output_test/bids_csv.csv # outpath bids.csv\n",
    "# pipeline\n",
    "qc_run_pipeline --bids_dir /home/jaimebarranco/Desktop/samples_v3_bids --out_path /home/jaimebarranco/Desktop/fetal_qc_output/ --mask-patterns-base /home/jaimebarranco/Desktop/samples_v3_bids/derivatives/masks/ --mask-patterns \"sub-{subject}_mask.nii.gz\" --suffix T1w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MREye-QC - brain/eye masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# mreye-qc\n",
    "docker run -it --rm -v /home/jaimebarranco/Desktop/samples_v3_bids:/data:ro -v /home/jaimebarranco/Desktop/mreyeqc_output:/out mreyeqc:latest /data /out participant --participant_label 001 --nprocs 12 --omp-nthreads 12\n",
    "# 7bd99c588704826d399898ec1f7419d40dc09b27473d511066f3cf8ab097fe5a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# def run_command(command):\n",
    "#     process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "#     console_output, console_errors = process.communicate()\n",
    "\n",
    "def run_command(command):\n",
    "    os.system(command)\n",
    "\n",
    "input_folder = '/home/jaimebarranco/Desktop/samples_v3_bids'\n",
    "output_folder = '/home/jaimebarranco/Desktop/mreyeqc_output'\n",
    "n_procs = 12\n",
    "n_threads = 12\n",
    "\n",
    "n_sub = [f.path for f in os.scandir(input_folder) if f.is_dir() and f.name.startswith('sub-')] # number of subjects in input folder\n",
    "n_sub = len(n_sub)\n",
    "print(f'Number of subjects: {n_sub}')\n",
    "\n",
    "# Run MRIQC for each subject\n",
    "for sub in range(1, 6):\n",
    "    command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out participant --participant_label {sub+1:03} --nprocs {n_procs} --omp-nthreads {n_threads}'\n",
    "    print(command)\n",
    "    run_command(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run MRIQC group analysis\n",
    "command = f'docker run --rm -v {input_folder}:/data:ro -v {output_folder}:/out mreyeqc_test:latest /data /out group'\n",
    "print(command)\n",
    "run_command(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation - old Meri's scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mriqc_eye_mask')\n",
    "\n",
    "# metrics\n",
    "meri = df['quality'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [meri, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_name = [i for i in df.columns if i not in ['subject', 'sub number', 'comments', 'Sex', 'Age', 'Height', 'Weight', 'BMI', 'axial_length']]\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "for i in range(len(metrics)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA:\n",
    "        print(f'{metrics_name[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# plot Meri vs. all the metrics\n",
    "# for i in range(len(metrics)):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.scatter(meri, metrics[i])\n",
    "#     plt.xlabel('Meri')\n",
    "#     plt.ylabel(metrics_name[i])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% confidence interval: snr_csf, summary_csf_mean, summary_wm_mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects outside the 95% confidence interval for qi_2\n",
    "relevant_metrics = [snr_csf, summary_csf_mean, summary_wm_mad]\n",
    "relevant_metrics_name = ['snr_csf', 'summary_csf_mean', 'summary_wm_mad']\n",
    "\n",
    "for i in range(len(relevant_metrics)):\n",
    "    print(f'\\n{relevant_metrics_name[i]}')\n",
    "    mean = np.mean(relevant_metrics[i])\n",
    "    std = np.std(relevant_metrics[i])\n",
    "    lower = mean - 2*std\n",
    "    upper = mean + 2*std\n",
    "    print(f'Lower: {lower}, Upper: {upper}')\n",
    "    for j in range(len(relevant_metrics[i])):\n",
    "        if relevant_metrics[i][j] < lower or relevant_metrics[i][j] > upper:\n",
    "            print(f'{j+1:03} - {relevant_metrics[i][j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "# correlation between Meri and Bene\n",
    "for i in range(len(metrics_meri)):\n",
    "    # R-squared measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    r_squared, p_value = stats.pearsonr(metrics_meri[i], metrics_bene[i])\n",
    "    # if p_value < ALPHA:\n",
    "    print(f'{metrics_names[i]}: r^2 = {r_squared:.4}, p-value = {p_value:.4}')\n",
    "\n",
    "# Eyes closed correlation\n",
    "sum = 0\n",
    "for i in range(len(eyes_closed_meri)):\n",
    "    if eyes_closed_meri[i] == eyes_closed_bene[i]: sum += 1 \n",
    "print(f'eyes_closed: {np.round(sum/len(eyes_closed_meri)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical pie charts Meri - Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating_text'].to_list()\n",
    "rating_meri_exclude = rating_meri.count('exclude')\n",
    "rating_meri_poor = rating_meri.count('poor')\n",
    "rating_meri_acceptable = rating_meri.count('acceptable')\n",
    "rating_meri_excellent = rating_meri.count('excellent')\n",
    "rating_meri_count = [rating_meri_exclude, rating_meri_poor, rating_meri_acceptable, rating_meri_excellent]\n",
    "blur_meri = df_meri['blur_text'].to_list()\n",
    "blur_meri_low = blur_meri.count('low')\n",
    "blur_meri_moderate = blur_meri.count('moderate')\n",
    "blur_meri_high = blur_meri.count('high')\n",
    "blur_meri_count = [blur_meri_low, blur_meri_moderate, blur_meri_high]\n",
    "noise_meri = df_meri['noise_text'].to_list()\n",
    "noise_meri_low = noise_meri.count('low')\n",
    "noise_meri_moderate = noise_meri.count('moderate')\n",
    "noise_meri_high = noise_meri.count('high')\n",
    "noise_meri_count = [noise_meri_low, noise_meri_moderate, noise_meri_high]\n",
    "motion_meri = df_meri['motion_text'].to_list()\n",
    "motion_meri_low = motion_meri.count('low')\n",
    "motion_meri_moderate = motion_meri.count('moderate')\n",
    "motion_meri_high = motion_meri.count('high')\n",
    "motion_meri_count = [motion_meri_low, motion_meri_moderate, motion_meri_high]\n",
    "bgair_meri = df_meri['bgair_text'].to_list()\n",
    "bgair_meri_low = bgair_meri.count('low')\n",
    "bgair_meri_moderate = bgair_meri.count('moderate')\n",
    "bgair_meri_high = bgair_meri.count('high')\n",
    "bgair_meri_count = [bgair_meri_low, bgair_meri_moderate, bgair_meri_high]\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating_text'].to_list()\n",
    "rating_bene_exclude = rating_bene.count('exclude')\n",
    "rating_bene_poor = rating_bene.count('poor')\n",
    "rating_bene_acceptable = rating_bene.count('acceptable')\n",
    "rating_bene_excellent = rating_bene.count('excellent')\n",
    "rating_bene_count = [rating_bene_exclude, rating_bene_poor, rating_bene_acceptable, rating_bene_excellent]\n",
    "blur_bene = df_bene['blur_text'].to_list()\n",
    "blur_bene_low = blur_bene.count('low')\n",
    "blur_bene_moderate = blur_bene.count('moderate')\n",
    "blur_bene_high = blur_bene.count('high')\n",
    "blur_bene_count = [blur_bene_low, blur_bene_moderate, blur_bene_high]\n",
    "noise_bene = df_bene['noise_text'].to_list()\n",
    "noise_bene_low = noise_bene.count('low')\n",
    "noise_bene_moderate = noise_bene.count('moderate')\n",
    "noise_bene_high = noise_bene.count('high')\n",
    "noise_bene_count = [noise_bene_low, noise_bene_moderate, noise_bene_high]\n",
    "motion_bene = df_bene['motion_text'].to_list()\n",
    "motion_bene_low = motion_bene.count('low')\n",
    "motion_bene_moderate = motion_bene.count('moderate')\n",
    "motion_bene_high = motion_bene.count('high')\n",
    "motion_bene_count = [motion_bene_low, motion_bene_moderate, motion_bene_high]\n",
    "bgair_bene = df_bene['bgair_text'].to_list()\n",
    "bgair_bene_low = bgair_bene.count('low')\n",
    "bgair_bene_moderate = bgair_bene.count('moderate')\n",
    "bgair_bene_high = bgair_bene.count('high')\n",
    "bgair_bene_count = [bgair_bene_low, bgair_bene_moderate, bgair_bene_high]\n",
    "\n",
    "# Categories\n",
    "categories4 = ['exclude', 'poor', 'acceptable', 'excellent']\n",
    "categories3 = ['low', 'moderate', 'high']\n",
    "\n",
    "# Color palette\n",
    "# custom_colors = ['#F8D948', '#B847C4', '#4C9F38', '#3F75AA']\n",
    "custom_colors_4 = ['#dc3545', '#ffc107', '#0d6efd', '#198754']\n",
    "custom_colors_3 = ['#198754', '#ffc107', '#dc3545']\n",
    "\n",
    "# Pie chart - ratings\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(rating_meri_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(rating_bene_count, labels=categories4, autopct='%1.1f%%', startangle=140, colors=custom_colors_4)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Ratings')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# # Bar chart - ratings\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# ax[0].bar(categories4, rating_meri_count, color=custom_colors_4)\n",
    "# ax[0].set_title('Meri')\n",
    "# ax[1].bar(categories4, rating_bene_count, color=custom_colors_4)\n",
    "# ax[1].set_title('Bene')\n",
    "\n",
    "# # Adding title to the figure\n",
    "# fig.suptitle('Bar Chart - Ratings')\n",
    "# fig.tight_layout()\n",
    "# fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - blur\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(blur_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(blur_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Blur')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - noise\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(noise_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(noise_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Noise')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - motion\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(motion_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(motion_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Motion')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Pie chart - bgair\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "ax[0].pie(bgair_meri_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[0].set_title('Meri')\n",
    "ax[1].pie(bgair_bene_count, labels=categories3, autopct='%1.1f%%', startangle=140, colors=custom_colors_3)\n",
    "ax[1].set_title('Bene')\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Adding title to the figure\n",
    "fig.suptitle('Pie Chart - Background Air')\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots Meri & Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "df_meri = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/subjective_evaluations/SHIP_QCmri_MBC/ratings.csv')\n",
    "df_bene = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/subjective_evaluations/Evaluations_frb/ratings.csv')\n",
    "\n",
    "# metrics meri\n",
    "rating_meri = df_meri['rating'].to_numpy()\n",
    "blur_meri = df_meri['blur'].to_numpy()\n",
    "noise_meri = df_meri['noise'].to_numpy()\n",
    "motion_meri = df_meri['motion'].to_numpy()\n",
    "bgair_meri = df_meri['bgair'].to_numpy()\n",
    "eyes_closed_meri = np.where(df_meri['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_meri = df_meri['selected_slices'].to_numpy()\n",
    "\n",
    "# metrics bene\n",
    "rating_bene = df_bene['rating'].to_numpy()\n",
    "blur_bene = df_bene['blur'].to_numpy()\n",
    "noise_bene = df_bene['noise'].to_numpy()\n",
    "motion_bene = df_bene['motion'].to_numpy()\n",
    "bgair_bene = df_bene['bgair'].to_numpy()\n",
    "eyes_closed_bene = np.where(df_bene['artifacts'].to_numpy() == \"['eyes-closed']\", 1, 0)\n",
    "# selected_slices_bene = df_bene['selected_slices'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics_meri = [rating_meri, blur_meri, noise_meri, motion_meri, bgair_meri, eyes_closed_meri] #, selected_slices_meri]\n",
    "metrics_bene = [rating_bene, blur_bene, noise_bene, motion_bene, bgair_bene, eyes_closed_bene] #, selected_slices_bene]\n",
    "metrics_names = ['rating', 'blur', 'noise', 'motion', 'bgair', 'eyes_closed'] #, 'selected_slices']\n",
    "\n",
    "# scatter plots of the metrics (Meri vs Bene)\n",
    "for i in range(len(metrics_meri)):\n",
    "    # Create a joint plot with marginal distributions\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    sns.set_palette(\"Set1\")  # Choose a color palette\n",
    "    # Create the joint plot\n",
    "    g = sns.jointplot(x=metrics_meri[i], y=metrics_bene[i], kind='scatter', color='b', s=40, edgecolor=\"skyblue\", linewidth=2)\n",
    "    # Annotate the axes with labels\n",
    "    g.ax_joint.set_xlabel(\"Meri\", fontsize=12)\n",
    "    g.ax_joint.set_ylabel(\"Bene\", fontsize=12)\n",
    "    plt.title(metrics_names[i])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation IQMs and subjective scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics correlation with avg ratings (Meri and Bene)\n",
    "\n",
    "Metrics whose p-values are lower than 0.05 are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# read an excel file in .xlsx format\n",
    "MASK = 'brain' # 'brain' or 'eye'\n",
    "ARTIFACT = 'rating' # rating, blur, noise, motion, bgair\n",
    "CRITERIA = 0 # 0 or 1\n",
    "EXCLUDE = 1 # True or False, 1 or 0 IF CRITERIA = 1\n",
    "\n",
    "if MASK == 'brain':\n",
    "    sheet_name = 'brainmask_avg_data'\n",
    "elif MASK == 'eye':\n",
    "    sheet_name = 'mreyeqc_eyemask_avg'\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name=sheet_name)\n",
    "\n",
    "# rating subgroup\n",
    "df_rating = df.groupby([\"rating_text\"])\n",
    "\n",
    "# exclusion criteria\n",
    "if CRITERIA:\n",
    "    if EXCLUDE:\n",
    "        df_rating_exclude = df_rating.get_group(\"Exclude\")\n",
    "        df = df_rating_exclude # not to rewrite everything\n",
    "    else:\n",
    "        df_rating_exclude = df_rating.get_group(\"Exclude\")\n",
    "        df_rating_include = df[~df.index.isin(df_rating_exclude.index)] # Get the opposite of the group labeled \"Exclude\"\n",
    "        df = df_rating_include # not to rewrite everything\n",
    "\n",
    "# metrics\n",
    "rating = df[f'{ARTIFACT}'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [rating, cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_names = [f'{ARTIFACT}', 'cjv', 'cnr', 'efc', 'fber', 'fwhm_avg', 'fwhm_x', 'fwhm_y', 'fwhm_z', 'icvs_csf', 'icvs_gm', 'icvs_wm', 'inu_med', 'inu_range', 'qi_1', 'qi_2', 'rpve_csf', 'rpve_gm', 'rpve_wm', 'snr_csf', 'snr_gm', 'snr_total', 'snr_wm', 'snrd_csf', 'snrd_gm', 'snrd_total', 'snrd_wm', 'summary_bg_k', 'summary_bg_mad', 'summary_bg_mean', 'summary_bg_median', 'summary_bg_n', 'summary_bg_p05', 'summary_bg_p95', 'summary_bg_stdv', 'summary_csf_k', 'summary_csf_mad', 'summary_csf_mean', 'summary_csf_median', 'summary_csf_n', 'summary_csf_p05', 'summary_csf_p95', 'summary_csf_stdv', 'summary_gm_k', 'summary_gm_mad', 'summary_gm_mean', 'summary_gm_median', 'summary_gm_n', 'summary_gm_p05', 'summary_gm_p95', 'summary_gm_stdv', 'summary_wm_k', 'summary_wm_mad', 'summary_wm_mean', 'summary_wm_median', 'summary_wm_n', 'summary_wm_p05', 'summary_wm_p95', 'summary_wm_stdv', 'tpm_overlap_csf', 'tpm_overlap_gm', 'tpm_overlap_wm', 'wm2max']\n",
    "\n",
    "# correlation between Meri and all the metrics\n",
    "ALPHA = 0.05 # p_values below this value are considered significant\n",
    "metrics_dict = {} # Create a dictionary of arrays using a loop\n",
    "\n",
    "# Check if rating follows a normal distribution (Shapiro test)\n",
    "print(f'Rating: {stats.shapiro(rating)}')\n",
    "\n",
    "# plot rating distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.hist(rating, bins=4)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(metrics)):\n",
    "    # R measures the proportion of the variance in one variable that is predictable from the other variable.\n",
    "    # p_value is a measure of the evidence against a null hypothesis. A small p-value indicates strong evidence against the null hypothesis, while a large p-value indicates weak evidence against the null hypothesis \n",
    "    # null hypothesis: x comes from a normal distribution\n",
    "    # if p_value < alpha: The null hypothesis can be rejected --> The set may NOT follow a normal distribution\n",
    "    statistic, p_shapiro = stats.shapiro(metrics[i].flatten()) # check if it's a normal distribution\n",
    "    r, p_value = stats.pearsonr(rating, metrics[i])\n",
    "    if p_value < ALPHA and p_shapiro > ALPHA:\n",
    "        metrics_dict[metrics_names[i]] = metrics[i]\n",
    "        print(f'{metrics_names[i]}: r = {r:.4}, p-value = {p_value:.4} || The set may follow a normal distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI95 - Subjects\n",
    "\n",
    "From the significant metrics extracted above, extract the outliers and see possible correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in metrics_dict.items() :\n",
    "    if key != f'{ARTIFACT}':\n",
    "        print(f'\\n{key}')\n",
    "        mean = np.mean(value)\n",
    "        std = np.std(value)\n",
    "        lower = mean - 2*std\n",
    "        upper = mean + 2*std\n",
    "        print(f'Lower: {lower}, Upper: {upper}')\n",
    "        for i in range(len(value)):\n",
    "            if value[i] < lower or value[i] > upper:\n",
    "                num_sub = df['sub'].values[i]\n",
    "                name_sub = df['name'].values[i].split('-')[1]\n",
    "                print(f'{num_sub:03} - {name_sub} - {value[i]} - {rating[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors of bad quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "train_test = True\n",
    "\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='mreyeqc_brainmask_avg')\n",
    "\n",
    "target = 'rating'\n",
    "\n",
    "# metrics\n",
    "rating = df[f'{target}'].to_numpy()\n",
    "cjv = df['cjv'].to_numpy()\n",
    "cnr = df['cnr'].to_numpy()\n",
    "efc = df['efc'].to_numpy()\n",
    "fber = df['fber'].to_numpy()\n",
    "fwhm_avg = df['fwhm_avg'].to_numpy()\n",
    "fwhm_x = df['fwhm_x'].to_numpy()\n",
    "fwhm_y = df['fwhm_y'].to_numpy()\n",
    "fwhm_z = df['fwhm_z'].to_numpy()\n",
    "icvs_csf = df['icvs_csf'].to_numpy()\n",
    "icvs_gm = df['icvs_gm'].to_numpy()\n",
    "icvs_wm = df['icvs_wm'].to_numpy()\n",
    "inu_med = df['inu_med'].to_numpy()\n",
    "inu_range = df['inu_range'].to_numpy()\n",
    "qi_1 = df['qi_1'].to_numpy()\n",
    "qi_2 = df['qi_2'].to_numpy()\n",
    "rpve_csf = df['rpve_csf'].to_numpy()\n",
    "rpve_gm = df['rpve_gm'].to_numpy()\n",
    "rpve_wm = df['rpve_wm'].to_numpy()\n",
    "snr_csf = df['snr_csf'].to_numpy()\n",
    "snr_gm = df['snr_gm'].to_numpy()\n",
    "snr_total = df['snr_total'].to_numpy()\n",
    "snr_wm = df['snr_wm'].to_numpy()\n",
    "snrd_csf = df['snrd_csf'].to_numpy()\n",
    "snrd_gm = df['snrd_gm'].to_numpy()\n",
    "snrd_total = df['snrd_total'].to_numpy()\n",
    "snrd_wm = df['snrd_wm'].to_numpy()\n",
    "summary_bg_k = df['summary_bg_k'].to_numpy()\n",
    "summary_bg_mad = df['summary_bg_mad'].to_numpy()\n",
    "summary_bg_mean = df['summary_bg_mean'].to_numpy()\n",
    "summary_bg_median = df['summary_bg_median'].to_numpy()\n",
    "summary_bg_n = df['summary_bg_n'].to_numpy()\n",
    "summary_bg_p05 = df['summary_bg_p05'].to_numpy()\n",
    "summary_bg_p95 = df['summary_bg_p95'].to_numpy()\n",
    "summary_bg_stdv = df['summary_bg_stdv'].to_numpy()\n",
    "summary_csf_k = df['summary_csf_k'].to_numpy()\n",
    "summary_csf_mad = df['summary_csf_mad'].to_numpy()\n",
    "summary_csf_mean = df['summary_csf_mean'].to_numpy()\n",
    "summary_csf_median = df['summary_csf_median'].to_numpy()\n",
    "summary_csf_n = df['summary_csf_n'].to_numpy()\n",
    "summary_csf_p05 = df['summary_csf_p05'].to_numpy()\n",
    "summary_csf_p95 = df['summary_csf_p95'].to_numpy()\n",
    "summary_csf_stdv = df['summary_csf_stdv'].to_numpy()\n",
    "summary_gm_k = df['summary_gm_k'].to_numpy()\n",
    "summary_gm_mad = df['summary_gm_mad'].to_numpy()\n",
    "summary_gm_mean = df['summary_gm_mean'].to_numpy()\n",
    "summary_gm_median = df['summary_gm_median'].to_numpy()\n",
    "summary_gm_n = df['summary_gm_n'].to_numpy()\n",
    "summary_gm_p05 = df['summary_gm_p05'].to_numpy()\n",
    "summary_gm_p95 = df['summary_gm_p95'].to_numpy()\n",
    "summary_gm_stdv = df['summary_gm_stdv'].to_numpy()\n",
    "summary_wm_k = df['summary_wm_k'].to_numpy()\n",
    "summary_wm_mad = df['summary_wm_mad'].to_numpy()\n",
    "summary_wm_mean = df['summary_wm_mean'].to_numpy()\n",
    "summary_wm_median = df['summary_wm_median'].to_numpy()\n",
    "summary_wm_n = df['summary_wm_n'].to_numpy()\n",
    "summary_wm_p05 = df['summary_wm_p05'].to_numpy()\n",
    "summary_wm_p95 = df['summary_wm_p95'].to_numpy()\n",
    "summary_wm_stdv = df['summary_wm_stdv'].to_numpy()\n",
    "tpm_overlap_csf = df['tpm_overlap_csf'].to_numpy()\n",
    "tpm_overlap_gm = df['tpm_overlap_gm'].to_numpy()\n",
    "tpm_overlap_wm = df['tpm_overlap_wm'].to_numpy()\n",
    "wm2max = df['wm2max'].to_numpy()\n",
    "\n",
    "# add all the metrics above to a list\n",
    "metrics = [cjv, cnr, efc, fber, fwhm_avg, fwhm_x, fwhm_y, fwhm_z, icvs_csf, icvs_gm, icvs_wm, inu_med, inu_range, qi_1, qi_2, rpve_csf, rpve_gm, rpve_wm, snr_csf, snr_gm, snr_total, snr_wm, snrd_csf, snrd_gm, snrd_total, snrd_wm, summary_bg_k, summary_bg_mad, summary_bg_mean, summary_bg_median, summary_bg_n, summary_bg_p05, summary_bg_p95, summary_bg_stdv, summary_csf_k, summary_csf_mad, summary_csf_mean, summary_csf_median, summary_csf_n, summary_csf_p05, summary_csf_p95, summary_csf_stdv, summary_gm_k, summary_gm_mad, summary_gm_mean, summary_gm_median, summary_gm_n, summary_gm_p05, summary_gm_p95, summary_gm_stdv, summary_wm_k, summary_wm_mad, summary_wm_mean, summary_wm_median, summary_wm_n, summary_wm_p05, summary_wm_p95, summary_wm_stdv, tpm_overlap_csf, tpm_overlap_gm, tpm_overlap_wm, wm2max]\n",
    "metrics_names = ['cjv', 'cnr', 'efc', 'fber', 'fwhm_avg', 'fwhm_x', 'fwhm_y', 'fwhm_z', 'icvs_csf', 'icvs_gm', 'icvs_wm', 'inu_med', 'inu_range', 'qi_1', 'qi_2', 'rpve_csf', 'rpve_gm', 'rpve_wm', 'snr_csf', 'snr_gm', 'snr_total', 'snr_wm', 'snrd_csf', 'snrd_gm', 'snrd_total', 'snrd_wm', 'summary_bg_k', 'summary_bg_mad', 'summary_bg_mean', 'summary_bg_median', 'summary_bg_n', 'summary_bg_p05', 'summary_bg_p95', 'summary_bg_stdv', 'summary_csf_k', 'summary_csf_mad', 'summary_csf_mean', 'summary_csf_median', 'summary_csf_n', 'summary_csf_p05', 'summary_csf_p95', 'summary_csf_stdv', 'summary_gm_k', 'summary_gm_mad', 'summary_gm_mean', 'summary_gm_median', 'summary_gm_n', 'summary_gm_p05', 'summary_gm_p95', 'summary_gm_stdv', 'summary_wm_k', 'summary_wm_mad', 'summary_wm_mean', 'summary_wm_median', 'summary_wm_n', 'summary_wm_p05', 'summary_wm_p95', 'summary_wm_stdv', 'tpm_overlap_csf', 'tpm_overlap_gm', 'tpm_overlap_wm', 'wm2max']\n",
    "\n",
    "# Generate the training set.  Set random_state to be able to replicate results.\n",
    "if train_test:\n",
    "    train = df.sample(frac=0.8, random_state=1)\n",
    "else: \n",
    "    train = df    \n",
    "\n",
    "# Select anything not in the training set and put it in the testing set.\n",
    "test = df.loc[~df.index.isin(train.index)]\n",
    "\n",
    "# Print the shapes of both sets.\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "ALPHA = 0.05\n",
    "f_statistics, p_values = f_regression(train[metrics_names], train[target])\n",
    "\n",
    "# Print for which metrics_names the p-value is less than ALPHA\n",
    "for i in range(len(metrics_names)):\n",
    "    if p_values[i] < ALPHA:\n",
    "        print(metrics_names[i], f_statistics[i], p_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear regression\n",
    "\n",
    "# Import the linear models.\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sma\n",
    "\n",
    "# Initialize the model class.\n",
    "model = linear_model.LinearRegression()\n",
    "# model = linear_model.Ridge(alpha = 0.5)\n",
    "\n",
    "# Fit the model to the training data.\n",
    "# feature = train[[\"summary_wm_k\", \"icvs_csf\", \"summary_csf_p05\", \"tpm_overlap_csf\", \"summary_csf_median\", \"summary_wm_median\", \"snr_total\", \"summary_csf_mad\", \"summary_wm_k\", \"summary_wm_mean\", \"summary_wm_p95\", \"cjv\", \"cnr\", \"summary_gm_p95\"]]\n",
    "feature = train[globals()['features']]\n",
    "trained_model = model.fit(feature, train[target])\n",
    "\n",
    "# Model score, intercept and  slope\n",
    "intercept = trained_model.intercept_\n",
    "slope = trained_model.coef_\n",
    "print(f'R score = {trained_model.score(feature, train[target])} \\nIntercept = {intercept} \\nSlope = {slope}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Error\n",
    "\n",
    "# Import the scikit-learn function to compute error. Explained variance score.\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "if train_test:\n",
    "    # Generate our predictions for the test set.\n",
    "    # predictions = model.predict(test[[\"summary_wm_k\", \"icvs_csf\", \"summary_csf_p05\", \"tpm_overlap_csf\", \"summary_csf_median\", \"summary_wm_median\", \"snr_total\", \"summary_csf_mad\", \"summary_wm_k\", \"summary_wm_mean\", \"summary_wm_p95\", \"cjv\", \"cnr\", \"summary_gm_p95\"]])\n",
    "    predictions = model.predict(test[globals()['features']])\n",
    "\n",
    "    # Compute error between our test predictions and the actual values.\n",
    "    RMSE = mean_squared_error(predictions, test[target], squared=False)\n",
    "    print('Root mean squared error: ', RMSE)\n",
    "\n",
    "    # R score (it can be negative, if the model is arbitrarily worse)\n",
    "    print('Variance score (R-score): %.2f' % r2_score(test[target], predictions))\n",
    "\n",
    "else:\n",
    "    # Generate our predictions for the test set.\n",
    "    predictions = model.predict(train[[\"summary_wm_k\"]])\n",
    "\n",
    "    # Compute error between our test predictions and the actual values.\n",
    "    RMSE = mean_squared_error(predictions, train[target], squared=False)\n",
    "    print('Root mean squared error: ', RMSE)\n",
    "\n",
    "    # R score\n",
    "    print('Variance score (R-score): %.2f' % r2_score(train[target], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "if train_test:\n",
    "    plt.scatter(x=test[target], y=predictions,  color='red')\n",
    "else:\n",
    "    plt.scatter(x=train[target], y=predictions, color='red')\n",
    "plt.title('Linear Regression - Test Set')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Predicted Rating')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "y = df['rating']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=0)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestRegressor(n_estimators=100) #, random_state=0) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_r2 = scores.mean()\n",
    "std_r2 = scores.std()\n",
    "print(\"Mean R2 Score:\", mean_r2)\n",
    "print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# # Plot ROC curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC)')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestRegressor as Binary Classifier\n",
    "\n",
    "Convert the continuous values into Exclude/Include binary classes {0, 1} after predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "y = df['rating']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=0)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestRegressor(n_estimators=100) #, random_state=0) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", scores)\n",
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_r2 = scores.mean()\n",
    "std_r2 = scores.std()\n",
    "print(\"Mean R2 Score:\", mean_r2)\n",
    "print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n",
    "# Define a threshold to classify instances\n",
    "threshold = 1  # Adjust this threshold as needed\n",
    "# Convert predicted values to binary classes using the threshold\n",
    "y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "# Evaluate the binary classification\n",
    "y_test_binary = (y_test < threshold).astype(int)\n",
    "# Get predicted probabilities\n",
    "# y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "# accuracy\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "confusion = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_binary) # not with the probabilities...\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier\n",
    "\n",
    "Change target to binary classification (0: Exclude, 1: Include). Then I can plot the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['exclusion']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "# Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=random_state) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "# - CROSS-VALIDATION - #\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "cross_val_scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='accuracy')\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-Validation R2 Scores:\", cross_val_scores)\n",
    "# Calculate the mean accuracy from cross-validation\n",
    "mean_accuracy = cross_val_scores.mean()\n",
    "print(\"Mean Accuracy from Cross-Validation:\", mean_accuracy)\n",
    "# ------------------- #\n",
    "\n",
    "# - TRAIN THE MODEL - #\n",
    "# Fit the model to the training data. Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# ------------------- #\n",
    "\n",
    "# - FEATURE IMPORTANCE - #\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Create a DataFrame to better visualize feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "# Display the sorted feature importances\n",
    "print(feature_importance_df)\n",
    "# ---------------------- #\n",
    "\n",
    "# -EVALUATE THE MODEL - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "# Get predicted probabilities\n",
    "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n",
    "# --------------------- #\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation (StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name', 'sub', 'subject', 'rating', 'rating_text', 'blur', 'blur_text', 'noise', 'noise_text', 'motion', 'motion_text', 'bgair', 'bgair_text', 'exclusion'])\n",
    "y = df['exclusion']\n",
    "\n",
    "# Create a Random Forest Classifier with random_state\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "\n",
    "# Perform cross-validation\n",
    "num_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Initialize lists to store ROC curve values and AUC scores\n",
    "roc_curves = []\n",
    "auc_scores = []\n",
    "\n",
    "# Iterate over each fold and calculate ROC curve and AUC\n",
    "for train_idx, test_idx in StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predicted probabilities of the positive class\n",
    "    y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC for this fold\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "# Plot ROC curves for each fold\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, (fpr, tpr) in enumerate(roc_curves):\n",
    "    plt.plot(fpr, tpr, lw=2, label='Fold %d (AUC = %0.2f)' % (i + 1, auc_scores[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Cross-Validation')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "target_names = ['excluded', 'included']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name', 'sub', 'subject', 'rating', 'rating_text', 'blur', 'blur_text', 'noise', 'noise_text', 'motion', 'motion_text', 'bgair', 'bgair_text', 'exclusion'])\n",
    "y = df['exclusion']\n",
    "\n",
    "# Create a Random Forest Classifier with random_state\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fold = 0\n",
    "for train_idx, test_idx in StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        rf_model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        name=f\"ROC fold {fold}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "        # plot_chance_level=(fold == n_splits - 1)\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "    fold+=1\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Positive label '{target_names[0]}')\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the feature importances (=1)\n",
    "print(feature_importances.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# avg\n",
    "mse_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "# Loop\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    # Create a Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=i) # random_state=0\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_model, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    mse_arr.append(mse)\n",
    "    # print(\"Mean Squared Error on Test Set:\", mse)\n",
    "    # Define a threshold to classify instances\n",
    "    threshold = 1  # Adjust this threshold as needed\n",
    "    # Convert predicted values to binary classes using the threshold\n",
    "    y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "    # Evaluate the binary classification\n",
    "    y_test_binary = (y_test < threshold).astype(int)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of {0, 1} class\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion)\n",
    "\n",
    "    # Dataframe for each iteration\n",
    "    globals()['feature_importance_df_%s' % i] = feature_importance_df\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "\n",
    "# Dataframe with the mean of the feature importances of all the iterations\n",
    "feature_importance_df_mean = pd.DataFrame({'Feature': X.columns, 'Importance': 0})\n",
    "for i in range(iter):\n",
    "    feature_importance_df_mean['Importance'] += globals()['feature_importance_df_%s' % i]['Importance']\n",
    "feature_importance_df_mean['Importance'] /= iter\n",
    "feature_importance_df_mean = feature_importance_df_mean.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df_mean)\n",
    "\n",
    "# results\n",
    "print('Mean of the mean squared error: ', np.mean(mse_arr))\n",
    "print('Mean of the accuracy: ', np.mean(acc_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the features with importance > 0.03 from the mean dataframe\n",
    "features_regressor = []\n",
    "for i in range(len(feature_importance_df_mean)):\n",
    "    if feature_importance_df_mean['Importance'].values[i] > 0.0:\n",
    "        features_regressor.append(feature_importance_df_mean['Feature'].values[i])\n",
    "print(features_regressor)\n",
    "\n",
    "first_features_regressor = features_regressor[:6]\n",
    "print(first_features_regressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['exclusion']\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# avg\n",
    "acc_arr = []\n",
    "\n",
    "# Loop\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    # Create a Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=i) # random_state=0\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_classifier.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    # y_pred_prob = rf_model.predict_proba(X_test)[:, 1]  # Probabilities of positive class\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    acc_arr.append(accuracy)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion)\n",
    "\n",
    "    # Dataframe for each iteration\n",
    "    globals()['feature_importance_df_%s' % i] = feature_importance_df\n",
    "\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "\n",
    "# Dataframe with the mean of the feature importances of all the iterations\n",
    "feature_importance_df_mean = pd.DataFrame({'Feature': X.columns, 'Importance': 0})\n",
    "for i in range(iter):\n",
    "    feature_importance_df_mean['Importance'] += globals()['feature_importance_df_%s' % i]['Importance']\n",
    "feature_importance_df_mean['Importance'] /= iter\n",
    "feature_importance_df_mean = feature_importance_df_mean.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df_mean)\n",
    "\n",
    "# results\n",
    "print('Mean accuracy: ', np.mean(acc_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the features with importance > 0.X from the mean dataframe\n",
    "features_classifier = []\n",
    "for i in range(len(feature_importance_df_mean)):\n",
    "    if feature_importance_df_mean['Importance'].values[i] > 0.03:\n",
    "        features_classifier.append(feature_importance_df_mean['Feature'].values[i])\n",
    "print(features_classifier)\n",
    "\n",
    "first_features_classifier = features_classifier[:7]\n",
    "print(first_features_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New random forest regressor with averaged importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "df['site'] = 'SHIP'\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_x = train_x.drop(columns=[\n",
    "    \"size_x\",\n",
    "    \"size_y\",\n",
    "    \"size_z\",\n",
    "    \"spacing_x\",\n",
    "    \"spacing_y\",\n",
    "    \"spacing_z\",\n",
    "    \"summary_bg_p05\", # all zeros\n",
    "])\n",
    "numeric_columns = train_x.columns.tolist()\n",
    "\n",
    "ratings = np.array([\"Exclude\"] * len(train_x))\n",
    "ratings[train_y.values < 1] = \"Exclude\"\n",
    "ratings[train_y.values >= 1] = \"Accept\"\n",
    "\n",
    "k_features = 6\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "sfs = SFS(\n",
    "    clf, \n",
    "    k_features=k_features, \n",
    "    forward=False, \n",
    "    verbose=0,\n",
    ")\n",
    "sfs = sfs.fit(train_x[numeric_columns], ratings)\n",
    "\n",
    "kept_features = list(sfs.k_feature_names_)\n",
    "print(f\"The {k_features} most important IQMs are {', '.join(sfs.k_feature_names_)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# random_state = 4\n",
    "# random_state = random.randint(0, 100)\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Loop\n",
    "mse_arr = []\n",
    "acc_arr = []\n",
    "prob_arr = []\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df[kept_features]\n",
    "    y = df['rating']\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    # Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=100, random_state=i) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_regressor, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - TRAIN THE MODEL - #\n",
    "    # Fit the model to the training data. Train the model\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_regressor.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "    # ---------------------- #\n",
    "\n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_regressor.predict(X_test)\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    mse_arr.append(mse)\n",
    "    # print(\"Mean Squared Error on Test Set:\", mse)\n",
    "    # Define a threshold to classify instances\n",
    "    threshold = 1  # Adjust this threshold as needed\n",
    "    # Convert predicted values to binary classes using the threshold\n",
    "    y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "    # Evaluate the binary classification\n",
    "    y_test_binary = (y_test < threshold).astype(int)\n",
    "    # Get predicted probabilities\n",
    "    # Convert predicted values to probabilities using the logistic function\n",
    "    # This approach might not be meaningful or appropriate, and it's usually better to use dedicated classification models or methods for computing class probabilities.\n",
    "    y_pred_prob = 1 - expit(y_pred_test)\n",
    "    prob_arr.append(y_pred_prob)\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    # print(\"Confusion Matrix:\\n\", confusion)\n",
    "    # --------------------- #\n",
    "\n",
    "avg_mse = np.mean(mse_arr)\n",
    "avg_acc = np.mean(acc_arr)\n",
    "avg_prob = np.mean(prob_arr, axis=0)\n",
    "print('Mean mse: ', avg_mse)\n",
    "print('Mean accuracy: ', avg_acc)\n",
    "print('Mean probs: ', avg_prob)\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plotting the Predicted vs Actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=1)\n",
    "plt.xlim(0, 4)\n",
    "plt.ylim(0, 4)\n",
    "plt.xlabel(\"Actual Ratings\")\n",
    "plt.ylabel(\"Predicted Ratings\")\n",
    "plt.title(\"Predicted vs Actual Ratings\")\n",
    "plt.show()\n",
    "# Plotting the Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.show()\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, avg_prob) # not with the probabilities...\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New random forest classifier with avg importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "y = df['rating']\n",
    "df['site'] = 'SHIP'\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_x = train_x.drop(columns=[\n",
    "    \"size_x\",\n",
    "    \"size_y\",\n",
    "    \"size_z\",\n",
    "    \"spacing_x\",\n",
    "    \"spacing_y\",\n",
    "    \"spacing_z\",\n",
    "    \"summary_bg_p05\", # all zeros\n",
    "])\n",
    "numeric_columns = train_x.columns.tolist()\n",
    "\n",
    "ratings = np.array([\"Exclude\"] * len(train_x))\n",
    "ratings[train_y.values < 1] = \"Exclude\"\n",
    "ratings[train_y.values >= 1] = \"Accept\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score to train_x and test_x for each column\n",
    "train_x_z = train_x.copy()\n",
    "test_x_z = test_x.copy()\n",
    "\n",
    "for col in numeric_columns:\n",
    "    mean = train_x[col].mean()\n",
    "    std = train_x[col].std()\n",
    "    train_x_z[col] = (train_x[col] - mean) / std\n",
    "    test_x_z[col] = (test_x[col] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_features = 6\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "sfs = SFS(\n",
    "    clf, \n",
    "    k_features=k_features, \n",
    "    forward=False, \n",
    "    verbose=0,\n",
    ")\n",
    "sfs = sfs.fit(train_x[numeric_columns], ratings)\n",
    "\n",
    "kept_features = list(sfs.k_feature_names_)\n",
    "print(f\"The {k_features} most important IQMs are {', '.join(sfs.k_feature_names_)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(sfs.k_feature_names_)\n",
    "\n",
    "#feature eliminated in order\n",
    "eliminated_feat = []\n",
    "for i in reversed(range(k_features, train_x[numeric_columns].shape[1]-1)):\n",
    "    feat0 = sfs.subsets_[i][\"feature_names\"]\n",
    "    feat1 = sfs.subsets_[i+1][\"feature_names\"]\n",
    "    el_feat = list(set(feat1) - set(feat0))\n",
    "    eliminated_feat.append(el_feat[0])\n",
    "\n",
    "print(\n",
    "    \"Here is the order in which the features have been eliminated \"\n",
    "    \"(1st in the list is the 1st feature to have been eliminated):\\n\"\n",
    "    f\"{', '.join(eliminated_feat)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept_features_aux = kept_features.copy()\n",
    "kept_features_aux.append(eliminated_feat[3])\n",
    "# kept_features_aux.pop(len(kept_features_aux)-1)\n",
    "print(f'{len(kept_features_aux)} features: {kept_features_aux}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "from scipy import stats\n",
    "\n",
    "# random_state = 0\n",
    "# random_state = random.randint(0, 100)\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'your_data.csv' with your actual data file)\n",
    "df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "\n",
    "# Loop\n",
    "acc_arr = []\n",
    "prob_arr = []\n",
    "std_arr = []\n",
    "iter = 10\n",
    "for i in range(iter):\n",
    "    # Separate features (X) and target (y)\n",
    "    # X = df[features]\n",
    "    X = df.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text','exclusion'])\n",
    "    y = df['exclusion'] # 1 if excluded, 0 if included\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    X_columns = X.columns.tolist()\n",
    "\n",
    "    # Z-score to train_x and test_x for each column\n",
    "    train_x_z = X_train.copy()\n",
    "    test_x_z = X_test.copy()\n",
    "    for col in X_columns:\n",
    "        mean_train = X_train[col].mean()\n",
    "        std_train = X_train[col].std()\n",
    "        mean_test = X_test[col].mean()\n",
    "        std_test = X_test[col].std()\n",
    "        train_x_z[col] = (X_train[col] - mean_train) / std_train\n",
    "        test_x_z[col] = (X_test[col] - mean_test) / std_test\n",
    "    \n",
    "    X_train = train_x_z\n",
    "    X_test = test_x_z\n",
    "    \n",
    "    X_train = X_train.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        \"summary_bg_p05\", # all zeros\n",
    "        \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "    train_columns = X_train.columns.tolist()\n",
    "\n",
    "    X_test = X_test.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        \"summary_bg_p05\", # all zeros\n",
    "        \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "\n",
    "    # Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=i) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - TRAIN THE MODEL - #\n",
    "    # Fit the model to the training data. Train the model\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "    # ---------------------- #\n",
    "\n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_classifier.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Probabilities of the positive class = excluded (1)\n",
    "    prob_arr.append(y_pred_prob)\n",
    "    # print(\"Probabilities of being class 1 (excluded):\", y_pred_prob)\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion)\n",
    "    # standard deviation\n",
    "    std = np.std(acc_arr)\n",
    "    std_arr.append(std)\n",
    "    print(\"Standard Deviation:\", std)\n",
    "    # --------------------- #\n",
    "\n",
    "avg_acc = np.mean(acc_arr)\n",
    "avg_prob = np.mean(prob_arr, axis=0)\n",
    "avg_std = np.mean(std_arr)\n",
    "print('Mean accuracy: ', avg_acc)\n",
    "print('Mean probs: ', avg_prob)\n",
    "print('Mean std: ', avg_std)\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, avg_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t')\n",
    "df_aux = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "subjects = df_aux['subject'].tolist()\n",
    "for i in range(len(subjects)):\n",
    "    subjects[i] = subjects[i].split('.')[0]\n",
    "# subdataframe of df with the subjects not in subjects\n",
    "df = df[~df['subject_name'].isin(subjects)] # pure testing set\n",
    "\n",
    "# - INFERENCE - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_regressor.predict(df[kept_features])\n",
    "# Define a threshold to classify instances\n",
    "threshold = 1  # Adjust this threshold as needed\n",
    "# Convert predicted values to binary classes using the threshold\n",
    "y_pred_binary = (y_pred_test < threshold).astype(int)\n",
    "\n",
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_regressor = pd.DataFrame({'Subject': df['subject_name'], 'Predicted rating': y_pred_test})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_regressor[df_predictions_regressor['Predicted rating'] < 1]\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_regressor)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# Load dataset into a pandas DataFrame \n",
    "df = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t')\n",
    "df = df.sort_values(by='bids_name') # order by df['bids_name']\n",
    "df_names = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset/bids_csv.csv', sep=',')\n",
    "df_names['bids_name'] = df_names['im'].str.split('/').str[-1].str.split('.').str[0]\n",
    "df_names = df_names.sort_values(by='bids_name') # order by df_names['sub']\n",
    "df['name'] = df_names['name']\n",
    "# move last column to second position\n",
    "cols = list(df)\n",
    "cols.insert(1, cols.pop(cols.index('name')))\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "subs_ls = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset'\n",
    "files_1 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs_ls)) if not f.startswith('.')]\n",
    "files_1 = [int(f) for f in files_1] # files from string to int\n",
    "numbers_1 = [f for f in range(1, len(files_1)+1)]\n",
    "df_1 = pd.DataFrame({'Subject': files_1, 'Number': numbers_1})\n",
    "\n",
    "subs = '/home/jaimebarranco/Downloads/samples_v3'\n",
    "files_2 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs)) if not f.startswith('.')]\n",
    "files_2 = [int(f) for f in files_2] # files from string to int\n",
    "numbers_2 = [f for f in range(1, len(files_2)+1)]\n",
    "df_2 = pd.DataFrame({'Subject': files_2, 'Number': numbers_2})\n",
    "\n",
    "# df_1 only with df_2 subjects\n",
    "df_3 = df_1[df_1['Subject'].isin(df_2['Subject'])]\n",
    "numbers_3 = df_3['Number']\n",
    "bids_names = [f'sub-{n:03}_T1w' for n in numbers_3]\n",
    "df_3['bids_name'] = bids_names # add bids names\n",
    "\n",
    "df_aux = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "df_aux['bids_name'] = bids_names\n",
    "# move last column to second position\n",
    "cols = list(df_aux)\n",
    "cols.insert(1, cols.pop(cols.index('bids_name')))\n",
    "df_aux = df_aux.loc[:, cols]\n",
    "\n",
    "# subdataframe of df without the bids_names in df_aux\n",
    "df = df[~df['bids_name'].isin(df_aux['bids_name'])] # pure testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - INFERENCE - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_classifier.predict(df[train_columns])\n",
    "\n",
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_classifier = pd.DataFrame({'subject_name': df['name'], 'bids_name': df['bids_name'], 'pred_rating': y_pred_test})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_classifier[df_predictions_classifier['pred_rating'] == 1]\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_classifier)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save excluded dataframe to excel\n",
    "excluded.to_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded.xlsx', sheet_name='excluded', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the reports of the excluded subject into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "reports_folder = '/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset'\n",
    "output_folder = '/home/jaimebarranco/Downloads/excluded_subjects'\n",
    "\n",
    "# copy html reports from reports_folder that match the subjects in excluded dataframe to output_folder\n",
    "for i in range(len(excluded)):\n",
    "    subject = excluded['Subject'].values[i]\n",
    "    for filename in os.listdir(reports_folder):\n",
    "        if filename.startswith(f'{subject}_report'):\n",
    "            shutil.copy(f'{reports_folder}/{filename}', f'{output_folder}/{filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy remaining subjects to Bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "import pandas as pd\n",
    "\n",
    "excluded_df = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded_edited.xlsx', sheet_name='excluded') # excluded (new ones)\n",
    "reports_folder = '/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset'\n",
    "output_folder = '/home/jaimebarranco/Downloads/excluded_subjects_bene'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# get the list of subjects with 'my_rate' as 'nan'\n",
    "subjects = excluded_df[excluded_df['my_rate'].isna()]['subject_name'].tolist()\n",
    "\n",
    "# loop through the reports folder and copy the html reports for the matching subjects to the output folder\n",
    "for file_name in os.listdir(reports_folder):\n",
    "    if file_name.endswith(\".html\"):\n",
    "        subject = file_name.split(\"_\")[0]\n",
    "        if subject in subjects:\n",
    "            shutil.copy(os.path.join(reports_folder, file_name), os.path.join(output_folder, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More pure testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633720/2592909095.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_3['bids_name'] = bids_names # add bids names\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# Load dataset into a pandas DataFrame \n",
    "df = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t')\n",
    "df = df.sort_values(by='bids_name') # order by df['bids_name']\n",
    "df_names = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset/bids_csv.csv', sep=',')\n",
    "df_names['bids_name'] = df_names['im'].str.split('/').str[-1].str.split('.').str[0]\n",
    "df_names = df_names.sort_values(by='bids_name') # order by df_names['sub']\n",
    "df['name'] = df_names['name']\n",
    "# move last column to second position\n",
    "cols = list(df)\n",
    "cols.insert(1, cols.pop(cols.index('name')))\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "subs_ls = '/mnt/sda1/Repos/a-eye/Data/SHIP_dataset/non_labeled_dataset'\n",
    "files_1 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs_ls)) if not f.startswith('.')]\n",
    "files_1 = [int(f) for f in files_1] # files from string to int\n",
    "numbers_1 = [f for f in range(1, len(files_1)+1)]\n",
    "df_1 = pd.DataFrame({'Subject': files_1, 'Number': numbers_1})\n",
    "\n",
    "subs = '/home/jaimebarranco/Downloads/samples_v3'\n",
    "files_2 = [(os.path.basename(f)).split('.')[0] for f in sorted(os.listdir(subs)) if not f.startswith('.')]\n",
    "files_2 = [int(f) for f in files_2] # files from string to int\n",
    "numbers_2 = [f for f in range(1, len(files_2)+1)]\n",
    "df_2 = pd.DataFrame({'Subject': files_2, 'Number': numbers_2})\n",
    "\n",
    "# df_1 only with df_2 subjects\n",
    "df_3 = df_1[df_1['Subject'].isin(df_2['Subject'])]\n",
    "numbers_3 = df_3['Number']\n",
    "bids_names = [f'sub-{n:03}_T1w' for n in numbers_3]\n",
    "df_3['bids_name'] = bids_names # add bids names\n",
    "\n",
    "df_aux = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data')\n",
    "df_aux['bids_name'] = bids_names\n",
    "# move last column to second position\n",
    "cols = list(df_aux)\n",
    "cols.insert(1, cols.pop(cols.index('bids_name')))\n",
    "df_aux = df_aux.loc[:, cols]\n",
    "\n",
    "# subdataframe of df without the bids_names in df_aux\n",
    "df = df[~df['bids_name'].isin(df_aux['bids_name'])] # pure testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the table with the metrics of the excluded subjects and the old ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/scores.xlsx', sheet_name='brainmask_avg_data') # old ones\n",
    "dfaux = pd.read_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/mriqc_non-labeled-dataset/group_T1w.tsv', sep='\\t') # IQMs\n",
    "df2 = pd.read_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded_edited.xlsx', sheet_name='excluded') # excluded (new ones)\n",
    "# remove subjects without 'my_rate'\n",
    "df2_filtered = df2[(~df2['my_rate'].isna()) & (df2['my_rate'] != '')]\n",
    "# dfaux only with the subjects in df2 in column 'bids_name'\n",
    "dfaux_filtered = dfaux[dfaux['bids_name'].isin(df2_filtered['bids_name'])]\n",
    "# reset 'index' column of dfaux_filtered to add ratings\n",
    "dfaux_filtered = dfaux_filtered.reset_index(drop=True)\n",
    "# insert 'my_rate' column from df2_filtered to dfaux_filtered\n",
    "dfaux_filtered['rating'] = df2_filtered['my_rate']\n",
    "cols = list(dfaux_filtered)\n",
    "cols.insert(1, cols.pop(cols.index('rating')))\n",
    "dfaux_filtered = dfaux_filtered.loc[:, cols]\n",
    "\n",
    "df1_aux = df1.drop(columns=['name','sub','subject','rating','rating_text','blur','blur_text','noise','noise_text','motion','motion_text','bgair','bgair_text'])\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "bids_names = df_3['bids_name']\n",
    "ratings = df1_aux['exclusion']\n",
    "df1_filtered = pd.DataFrame({'bids_name': bids_names, 'rating': ratings})\n",
    "# insert df1_aux into df1_filtered\n",
    "df1_aux = df1_aux.drop(columns=['exclusion'])\n",
    "df1_filtered = pd.concat([df1_filtered, df1_aux], axis=1)\n",
    "\n",
    "# join df1_filtered and dfaux_filtered\n",
    "df_merged = pd.concat([df1_filtered, dfaux_filtered], ignore_index=True)\n",
    "\n",
    "# large scale dataset = dfaux - df_merged\n",
    "df_ls_aux = dfaux.copy()\n",
    "df_ls_aux['name'] = df_names['name']\n",
    "df_ls = df_ls_aux[~df_ls_aux['bids_name'].isin(df_merged['bids_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'name' column from 'df'\n",
    "df_ls = df_ls.drop(columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ls['site'] = 'SHIP'\n",
    "# move 'site' column before 'cjv'\n",
    "cols = list(df_ls)\n",
    "cols.insert(1, cols.pop(cols.index('site')))\n",
    "df_ls = df_ls.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged to .tsv\n",
    "df_ls.to_csv('/home/jaimebarranco/Desktop/MRI-QC/mriqc/SHIP1027.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_merged.drop(columns=['rating','bids_name'])\n",
    "y = df_merged['rating']\n",
    "# df_merged['site'] = 'SHIP'\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Train/test split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "train_x = train_x.drop(columns=[\n",
    "    \"size_x\",\n",
    "    \"size_y\",\n",
    "    \"size_z\",\n",
    "    \"spacing_x\",\n",
    "    \"spacing_y\",\n",
    "    \"spacing_z\",\n",
    "    \"summary_bg_p05\", # all zeros\n",
    "])\n",
    "numeric_columns = train_x.columns.tolist()\n",
    "\n",
    "ratings = np.array([\"Exclude\"] * len(train_x))\n",
    "ratings[train_y.values < 1] = \"Exclude\"\n",
    "ratings[train_y.values >= 1] = \"Accept\"\n",
    "\n",
    "k_features = 5 # assuming 6 with the new dataset\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "sfs = SFS(\n",
    "    clf, \n",
    "    k_features=k_features, \n",
    "    forward=False, \n",
    "    verbose=0,\n",
    ")\n",
    "sfs = sfs.fit(train_x[numeric_columns], ratings)\n",
    "\n",
    "kept_features = list(sfs.k_feature_names_)\n",
    "print(f\"The {k_features} most important IQMs are {', '.join(sfs.k_feature_names_)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(sfs.k_feature_names_)\n",
    "\n",
    "#feature eliminated in order\n",
    "eliminated_feat = []\n",
    "for i in reversed(range(k_features, train_x[numeric_columns].shape[1]-1)):\n",
    "    feat0 = sfs.subsets_[i][\"feature_names\"]\n",
    "    feat1 = sfs.subsets_[i+1][\"feature_names\"]\n",
    "    el_feat = list(set(feat1) - set(feat0))\n",
    "    eliminated_feat.append(el_feat[0])\n",
    "\n",
    "print(\n",
    "    \"Here is the order in which the features have been eliminated \"\n",
    "    \"(1st in the list is the 1st feature to have been eliminated):\\n\"\n",
    "    f\"{', '.join(eliminated_feat)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kept_features_aux = kept_features.copy()\n",
    "kept_features_aux.append(eliminated_feat[3])\n",
    "# kept_features_aux.pop(len(kept_features_aux)-1)\n",
    "print(f'{len(kept_features_aux)} features: {kept_features_aux}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [ 9  2]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 8  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[26  3]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  5]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  5]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  5]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  2]\n",
      " [10  2]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[25  4]\n",
      " [ 5  3]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[24  7]\n",
      " [ 4  2]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[22  1]\n",
      " [13  1]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[24  4]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[25  0]\n",
      " [ 9  3]]\n",
      "Accuracy: 0.5405405405405406\n",
      "Confusion Matrix:\n",
      " [[19  2]\n",
      " [15  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[25  1]\n",
      " [ 9  2]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[21  8]\n",
      " [ 6  2]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [11  0]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[22  6]\n",
      " [ 8  1]]\n",
      "Accuracy: 0.8378378378378378\n",
      "Confusion Matrix:\n",
      " [[30  2]\n",
      " [ 4  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[25  2]\n",
      " [ 8  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  3]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[22  2]\n",
      " [12  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[27  3]\n",
      " [ 7  0]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[27  1]\n",
      " [ 9  0]]\n",
      "Accuracy: 0.7837837837837838\n",
      "Confusion Matrix:\n",
      " [[28  1]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  5]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[21  6]\n",
      " [ 6  4]]\n",
      "Accuracy: 0.5675675675675675\n",
      "Confusion Matrix:\n",
      " [[19  6]\n",
      " [10  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [12  0]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [10  1]]\n",
      "Accuracy: 0.5405405405405406\n",
      "Confusion Matrix:\n",
      " [[17  8]\n",
      " [ 9  3]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[26  1]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[21 10]\n",
      " [ 4  2]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[27  3]\n",
      " [ 6  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[26  3]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  2]\n",
      " [10  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[26  6]\n",
      " [ 4  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  3]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[21  3]\n",
      " [10  3]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[22 10]\n",
      " [ 4  1]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 8  1]]\n",
      "Accuracy: 0.7837837837837838\n",
      "Confusion Matrix:\n",
      " [[28  4]\n",
      " [ 4  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  5]\n",
      " [ 8  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  2]\n",
      " [10  1]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[25  2]\n",
      " [ 7  3]]\n",
      "Accuracy: 0.7837837837837838\n",
      "Confusion Matrix:\n",
      " [[27  1]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[23  5]\n",
      " [ 5  4]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  2]\n",
      " [10  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[25  2]\n",
      " [10  0]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[22  7]\n",
      " [ 6  2]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[24  3]\n",
      " [10  0]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[22  5]\n",
      " [ 7  3]]\n",
      "Accuracy: 0.5675675675675675\n",
      "Confusion Matrix:\n",
      " [[19  1]\n",
      " [15  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[23  2]\n",
      " [ 8  4]]\n",
      "Accuracy: 0.5405405405405406\n",
      "Confusion Matrix:\n",
      " [[20  7]\n",
      " [10  0]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  5]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 8  1]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  5]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [10  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[22  2]\n",
      " [10  3]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[26  3]\n",
      " [ 8  0]]\n",
      "Accuracy: 0.5945945945945946\n",
      "Confusion Matrix:\n",
      " [[22  4]\n",
      " [11  0]]\n",
      "Accuracy: 0.5945945945945946\n",
      "Confusion Matrix:\n",
      " [[19  2]\n",
      " [13  3]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[24  2]\n",
      " [10  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  0]\n",
      " [13  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [ 9  3]]\n",
      "Accuracy: 0.5945945945945946\n",
      "Confusion Matrix:\n",
      " [[21  3]\n",
      " [12  1]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[26  4]\n",
      " [ 5  2]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [10  1]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[24  4]\n",
      " [ 6  3]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[26  6]\n",
      " [ 3  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[27  2]\n",
      " [ 8  0]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[22  4]\n",
      " [ 9  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[23  5]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[23  4]\n",
      " [ 7  3]]\n",
      "Accuracy: 0.5945945945945946\n",
      "Confusion Matrix:\n",
      " [[22  6]\n",
      " [ 9  0]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[25  4]\n",
      " [ 8  0]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[26  2]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [10  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [12  0]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[24  1]\n",
      " [10  2]]\n",
      "Accuracy: 0.5945945945945946\n",
      "Confusion Matrix:\n",
      " [[21  1]\n",
      " [14  1]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[21  7]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[22  2]\n",
      " [ 9  4]]\n",
      "Accuracy: 0.5135135135135135\n",
      "Confusion Matrix:\n",
      " [[19  3]\n",
      " [15  0]]\n",
      "Accuracy: 0.7837837837837838\n",
      "Confusion Matrix:\n",
      " [[28  2]\n",
      " [ 6  1]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[24  0]\n",
      " [11  2]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[26  4]\n",
      " [ 6  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  6]\n",
      " [ 7  1]]\n",
      "Accuracy: 0.8108108108108109\n",
      "Confusion Matrix:\n",
      " [[27  3]\n",
      " [ 4  3]]\n",
      "Accuracy: 0.6216216216216216\n",
      "Confusion Matrix:\n",
      " [[22  0]\n",
      " [14  1]]\n",
      "Accuracy: 0.7027027027027027\n",
      "Confusion Matrix:\n",
      " [[25  2]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  4]\n",
      " [ 9  1]]\n",
      "Accuracy: 0.7567567567567568\n",
      "Confusion Matrix:\n",
      " [[26  1]\n",
      " [ 8  2]]\n",
      "Accuracy: 0.6756756756756757\n",
      "Confusion Matrix:\n",
      " [[22  4]\n",
      " [ 8  3]]\n",
      "Accuracy: 0.7297297297297297\n",
      "Confusion Matrix:\n",
      " [[25  3]\n",
      " [ 7  2]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  9]\n",
      " [ 4  1]]\n",
      "Accuracy: 0.6486486486486487\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [10  1]]\n",
      "Mean accuracy:  0.6816216216216214\n",
      "Mean probs:  [0.3147 0.2723 0.3085 0.2785 0.2997 0.2916 0.2902 0.2598 0.249  0.2867\n",
      " 0.281  0.2907 0.2755 0.2842 0.2675 0.3186 0.2848 0.2821 0.3098 0.2839\n",
      " 0.2679 0.2703 0.2728 0.2816 0.3048 0.2855 0.2968 0.3003 0.2734 0.2729\n",
      " 0.2711 0.2874 0.2784 0.2576 0.2998 0.2911 0.2666]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.special import expit  # For the logistic (sigmoid) function\n",
    "\n",
    "# random_state = 0\n",
    "# random_state = random.randint(0, 100)\n",
    "\n",
    "# Loop\n",
    "acc_arr = []\n",
    "prob_arr = []\n",
    "iter = 100\n",
    "for i in range(iter):\n",
    "    # Separate features (X) and target (y)\n",
    "    # X = df_merged[numeric_columns]\n",
    "    X = df_merged.drop(columns=['bids_name', 'rating'])\n",
    "    y = df_merged['rating'] # 1 if excluded, 0 if included\n",
    "\n",
    "    for col in X:\n",
    "        mean = X[col].mean()\n",
    "        std = X[col].std()\n",
    "        X[col] = (X[col] - mean) / std\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) # random_state=0\n",
    "\n",
    "    X_columns = X.columns.tolist()\n",
    "\n",
    "    X_train = X_train.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        \"summary_bg_p05\", # all zeros\n",
    "        \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "    train_columns = X_train.columns.tolist()\n",
    "\n",
    "    X_test = X_test.drop(columns=[\n",
    "        \"size_x\",\n",
    "        \"size_y\",\n",
    "        \"size_z\",\n",
    "        \"spacing_x\",\n",
    "        \"spacing_y\",\n",
    "        \"spacing_z\",\n",
    "        \"summary_bg_p05\", # all zeros\n",
    "        \"qi_1\", # almost all zeros\n",
    "    ])\n",
    "\n",
    "    # Create a Random Forest Regressor or Classifier (for classification tasks)\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=i) # random_state=0, bootstrap=True (If False, the whole dataset is used to build each tree.)\n",
    "\n",
    "    # - CROSS-VALIDATION - #\n",
    "    # Perform cross-validation\n",
    "    num_folds = 5  # Number of folds for cross-validation\n",
    "    scores = cross_val_score(rf_classifier, X_train, y_train, cv=num_folds, scoring='r2')\n",
    "    # Print the cross-validation scores for each fold\n",
    "    # print(\"Cross-Validation R2 Scores:\", scores)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    mean_r2 = scores.mean()\n",
    "    std_r2 = scores.std()\n",
    "    # print(\"Mean R2 Score:\", mean_r2)\n",
    "    # print(\"Standard Deviation of R2 Score:\", std_r2)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - TRAIN THE MODEL - #\n",
    "    # Fit the model to the training data. Train the model\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    # ------------------- #\n",
    "\n",
    "    # - FEATURE IMPORTANCE - #\n",
    "    # Get feature importances\n",
    "    feature_importances = rf_classifier.feature_importances_\n",
    "    # Create a DataFrame to better visualize feature importances\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "    # Sort features by importance in descending order\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    # Display the sorted feature importances\n",
    "    # print(feature_importance_df)\n",
    "    # ---------------------- #\n",
    "\n",
    "    # -EVALUATE THE MODEL - #\n",
    "    # Make predictions\n",
    "    y_pred_test = rf_classifier.predict(X_test)\n",
    "    # Get predicted probabilities\n",
    "    y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Probabilities of the positive class = excluded (1)\n",
    "    prob_arr.append(y_pred_prob)\n",
    "    # print(\"Probabilities of being class 1 (excluded):\", y_pred_prob)\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    acc_arr.append(accuracy)\n",
    "    confusion = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion)\n",
    "    # --------------------- #\n",
    "\n",
    "avg_acc = np.mean(acc_arr)\n",
    "avg_prob = np.mean(prob_arr, axis=0)\n",
    "print('Mean accuracy: ', avg_acc)\n",
    "print('Mean probs: ', avg_prob)\n",
    "\n",
    "# - PLOT PREDICTED VS ACTUAL VALUES - #\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, avg_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large scale testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilities + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = rf_classifier.predict_proba(df_ls[train_columns])[:, 1] # 1 for excluded according to the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "# how many values > 0.5 from y_scores\n",
    "threshold = 0.56\n",
    "count = (y_scores > threshold).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of values > threshold from y_scores\n",
    "y_pred_idx = (y_scores > threshold).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded subjects: 135/1027\n",
      "     subject_name    bids_name  pred_rating\n",
      "18      sub-IXWPL  sub-019_T1w         0.57\n",
      "35      sub-MEVDI  sub-036_T1w         0.57\n",
      "40      sub-RCROG  sub-041_T1w         0.57\n",
      "41      sub-TUBAD  sub-042_T1w         0.57\n",
      "53      sub-WNMXO  sub-054_T1w         0.57\n",
      "...           ...          ...          ...\n",
      "1174    sub-OSABR  sub-964_T1w         0.57\n",
      "1177    sub-KTHXT  sub-967_T1w         0.57\n",
      "1186    sub-BBWSL  sub-976_T1w         0.57\n",
      "1195    sub-ERUZQ  sub-985_T1w         0.57\n",
      "1201    sub-NIVUA  sub-991_T1w         0.57\n",
      "\n",
      "[135 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_classifier = pd.DataFrame({'subject_name': df_ls['name'], 'bids_name': df_ls['bids_name'], 'pred_rating': y_scores})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_classifier[df_predictions_classifier['pred_rating'] > threshold]\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_classifier)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded subjects: 923/1027\n",
      "     subject_name    bids_name  pred_rating\n",
      "1       sub-UPYHV  sub-002_T1w          1.0\n",
      "4       sub-DVKCK  sub-005_T1w          1.0\n",
      "5       sub-QAUMU  sub-006_T1w          1.0\n",
      "6       sub-PANUR  sub-007_T1w          1.0\n",
      "7       sub-CFPZJ  sub-008_T1w          1.0\n",
      "...           ...          ...          ...\n",
      "1203    sub-WXRXV  sub-993_T1w          1.0\n",
      "1204    sub-GFJAC  sub-994_T1w          1.0\n",
      "1205    sub-WXFZT  sub-995_T1w          1.0\n",
      "1206    sub-QPQCV  sub-996_T1w          1.0\n",
      "1207    sub-CJNZY  sub-997_T1w          1.0\n",
      "\n",
      "[923 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# - INFERENCE - #\n",
    "# Make predictions\n",
    "y_pred_test = rf_classifier.predict(df_ls[train_columns])\n",
    "\n",
    "# Create a DataFrame to better visualize the predictions\n",
    "df_predictions_classifier = pd.DataFrame({'subject_name': df_ls['name'], 'bids_name': df_ls['bids_name'], 'pred_rating': y_pred_test})\n",
    "# print(df_predictions)\n",
    "# Look for 0 values in the predictions\n",
    "excluded = df_predictions_classifier[df_predictions_classifier['pred_rating'] == 1]\n",
    "# number of excluded subjects\n",
    "print(f'excluded subjects: {len(excluded)}/{len(df_predictions_classifier)}')\n",
    "print(excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save excluded dataframe to excel\n",
    "excluded.to_excel('/home/jaimebarranco/Desktop/MRI-QC/excluded_N183_6feat.xlsx', sheet_name='excluded', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the reports of the excluded subject into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "reports_folder = '/home/jaimebarranco/Desktop/MRI-QC/fetal/fetalqc_non-labeled-dataset'\n",
    "output_folder = '/home/jaimebarranco/Downloads/excluded_N183_6feat'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# copy html reports from reports_folder that match the subjects in excluded dataframe to output_folder\n",
    "for i in range(len(excluded)):\n",
    "    subject = excluded['subject_name'].values[i]\n",
    "    for filename in os.listdir(reports_folder):\n",
    "        if filename.startswith(f'{subject}_report'):\n",
    "            shutil.copy(f'{reports_folder}/{filename}', f'{output_folder}/{filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-eye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
